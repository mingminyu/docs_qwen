{"config":{"lang":["en"],"separator":"[\\s\\u200b\\u3000\\-\u3001\u3002\uff0c\uff0e\uff1f\uff01\uff1b]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"\u901a\u4e49\u5343\u95ee 2","text":"<p>Qwen \u662f\u963f\u91cc\u5f00\u6e90\u51fa\u6765\u7684\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5927\u591a\u6a21\u6001\u6a21\u578b\u7cfb\u5217\uff0c\u76ee\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5df2\u7ecf\u5347\u7ea7\u5230 Qwen 2\u3002\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u6a21\u578b\u90fd\u662f\u5728\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u9ad8\u8d28\u91cf\u6570\u636e\u4e0a\u8fdb\u884c\u540e\u8bad\u7ec3 \uff0c\u4ee5\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002Qwen \u80fd\u591f\u8fdb\u884c \u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001 \u6587\u672c\u751f\u6210 \u3001 \u89c6\u89c9\u7406\u89e3 \u3001 \u97f3\u9891\u7406\u89e3 \u3001 \u5de5\u5177\u4f7f\u7528 \u3001 \u89d2\u8272\u626e\u6f14 \u4ee5\u53ca\u5145\u5f53 AI\u4ee3\u7406 \u7b49\u3002</p> <p>\u76ee\u524d Qwen 2 \u7248\u672c\u5177\u5907\u4ee5\u4e0b\u7279\u6027:</p> <ul> <li>\u63d0\u4f9b 6 \u79cd\u4e0d\u540c\u5927\u5c0f\u7684\u6a21\u578b: 0.5B\u30011.5B\u30014B\u30017B\u300114B\u300172B\uff1b</li> <li>\u6bcf\u79cd\u7c7b\u578b\u90fd\u5177\u5907 base \u6a21\u578b\u548c\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\uff0c\u5176\u4e2d\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u8fdb\u884c\u4e86\u5bf9\u9f50\uff1b</li> <li>\u57fa\u7840\u6a21\u578b\u548c\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u652f\u6301\u591a\u8bed\u8a00\uff1b</li> <li>\u6240\u6709\u6a21\u578b\u90fd\u7a33\u5b9a\u652f\u6301 32K \u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5bf9\u4e8e Qwen2 \u7684 7B \u4ee5\u53ca 72B \u6a21\u578b\u80fd\u6269\u5c55\u81f3 128K \u4e0a\u4e0b\u6587</li> <li>\u4f5c\u4e3a AI Agent\uff0cQwen \u652f\u6301\u5de5\u5177\u4f7f\u7528\u3001RAG\u3001\u89d2\u8272\u626e\u6f14\u3001\u626e\u6f14\uff1b</li> </ul>"},{"location":"blog/","title":"Blog","text":""},{"location":"v2/","title":"\u901a\u4e49\u5343\u95ee2","text":"<p>Qwen \u662f\u963f\u91cc\u5f00\u6e90\u51fa\u6765\u7684\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5927\u591a\u6a21\u6001\u6a21\u578b\u7cfb\u5217\uff0c\u76ee\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5df2\u7ecf\u5347\u7ea7\u5230 Qwen 2\u3002\u8bed\u8a00\u6a21\u578b\u548c\u591a\u6a21\u6001\u6a21\u578b\u90fd\u662f\u5728\u5927\u89c4\u6a21\u591a\u8bed\u8a00\u548c\u591a\u6a21\u6001\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u5728\u9ad8\u8d28\u91cf\u6570\u636e\u4e0a\u8fdb\u884c\u540e\u8bad\u7ec3 \uff0c\u4ee5\u66f4\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u3002Qwen \u80fd\u591f\u8fdb\u884c \u81ea\u7136\u8bed\u8a00\u7406\u89e3\u3001 \u6587\u672c\u751f\u6210 \u3001 \u89c6\u89c9\u7406\u89e3 \u3001 \u97f3\u9891\u7406\u89e3 \u3001 \u5de5\u5177\u4f7f\u7528 \u3001 \u89d2\u8272\u626e\u6f14 \u4ee5\u53ca\u5145\u5f53 AI\u4ee3\u7406 \u7b49\u3002</p> <p>\u76ee\u524d Qwen 2 \u7248\u672c\u5177\u5907\u4ee5\u4e0b\u7279\u6027:</p> <ul> <li>\u63d0\u4f9b 6 \u79cd\u4e0d\u540c\u5927\u5c0f\u7684\u6a21\u578b: 0.5B\u30011.5B\u30014B\u30017B\u300114B\u300172B\uff1b</li> <li>\u6bcf\u79cd\u7c7b\u578b\u90fd\u5177\u5907 base \u6a21\u578b\u548c\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\uff0c\u5176\u4e2d\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u8fdb\u884c\u4e86\u5bf9\u9f50\uff1b</li> <li>\u57fa\u7840\u6a21\u578b\u548c\u6307\u4ee4\u5fae\u8c03\u6a21\u578b\u652f\u6301\u591a\u8bed\u8a00\uff1b</li> <li>\u6240\u6709\u6a21\u578b\u90fd\u7a33\u5b9a\u652f\u6301 32K \u4e0a\u4e0b\u6587\u957f\u5ea6\uff0c\u5bf9\u4e8e Qwen2 \u7684 7B \u4ee5\u53ca 72B \u6a21\u578b\u80fd\u6269\u5c55\u81f3 128K \u4e0a\u4e0b\u6587</li> <li>\u4f5c\u4e3a AI Agent\uff0cQwen \u652f\u6301\u5de5\u5177\u4f7f\u7528\u3001RAG\u3001\u89d2\u8272\u626e\u6f14\u3001\u626e\u6f14\uff1b</li> </ul>"},{"location":"v2/awq/","title":"AWQ","text":"<p>\u5bf9\u4e8e\u91cf\u5316\u6a21\u578b\uff0c\u6211\u4eec\u63a8\u8350\u4f7f\u7528 AWQ \u7ed3\u5408 AutoAWQ \uff0c\u6fc0\u6d3b\u503c\u611f\u77e5\u7684\u6743\u91cd\u91cf\u5316\uff08Activation-aware Weight Quantization, AWQ\uff09\u662f\u4e00\u79cd\u9488\u5bf9 LLM \u7684\u4f4e\u6bd4\u7279\u6743\u91cd\u91cf\u5316\u7684\u786c\u4ef6\u53cb\u597d\u65b9\u6cd5\u3002\u800c AutoAWQ \u662f\u4e00\u4e2a\u6613\u4e8e\u4f7f\u7528\u7684\u5de5\u5177\u5305\uff0c\u7528\u4e8e 4 \u6bd4\u7279\u91cf\u5316\u6a21\u578b\u3002</p> <p>\u76f8\u8f83\u4e8e FP16\uff0cAutoAWQ \u80fd\u591f\u5c06\u6a21\u578b\u7684\u8fd0\u884c\u901f\u5ea6\u63d0\u5347 3 \u500d\uff0c\u5e76\u5c06\u5185\u5b58\u9700\u6c42\u964d\u4f4e\u81f3\u539f\u6765\u7684 \u2153\u3002AutoAWQ \u5b9e\u73b0\u4e86 AWQ \u7b97\u6cd5\uff0c\u53ef\u7528\u4e8e LLM \u7684\u91cf\u5316\u5904\u7406\u3002\u5728\u672c\u6587\u6863\u4e2d\uff0c\u6211\u4eec\u5c06\u5411\u60a8\u5c55\u793a\u5982\u4f55\u5728 Transformers \u6846\u67b6\u4e0b\u4f7f\u7528\u91cf\u5316\u6a21\u578b\uff0c\u4ee5\u53ca\u5982\u4f55\u5bf9\u60a8\u81ea\u5df1\u7684\u6a21\u578b\u8fdb\u884c\u91cf\u5316\u3002</p>"},{"location":"v2/awq/#1-transformers-awq","title":"1. \u5728 Transformers \u4e2d\u4f7f\u7528 AWQ \u91cf\u5316\u6a21\u578b","text":"<p>\u73b0\u5728\uff0cTransformers \u5df2\u7ecf\u6b63\u5f0f\u652f\u6301 AutoAWQ\uff0c\u8fd9\u610f\u5473\u7740\u60a8\u53ef\u4ee5\u76f4\u63a5\u5728 Transformers \u4e2d\u4f7f\u7528\u91cf\u5316\u6a21\u578b\u3002\u4ee5\u4e0b\u662f\u4e00\u4e2a\u975e\u5e38\u7b80\u5355\u7684\u4ee3\u7801\u7247\u6bb5\uff0c\u5c55\u793a\u5982\u4f55\u8fd0\u884c\u91cf\u5316\u6a21\u578b Qwen2-7B-Instruct-AWQ\uff1a</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ndevice = \"cuda\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2-7B-Instruct-AWQ\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct-AWQ\")\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] \n    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(\n    generated_ids, skip_special_tokens=True\n    )[0]\n</code></pre>"},{"location":"v2/awq/#2-vllm-awq","title":"2. \u5728 vLLM \u4e2d\u4f7f\u7528 AWQ \u91cf\u5316\u6a21\u578b","text":"<p>vLLM \u5df2\u7ecf\u652f\u6301\u4e86 AWQ\uff0c\u8fd9\u610f\u5473\u7740\u60a8\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u6211\u4eec\u63d0\u4f9b\u7684 AWQ \u6a21\u578b\uff0c\u6216\u8005\u662f\u901a\u8fc7 AutoAWQ \u8bad\u7ec3\u5f97\u5230\u7684\u4e0e vLLM \u517c\u5bb9\u7684\u6a21\u578b\u3002\u5b9e\u9645\u4e0a\uff0c\u5176\u7528\u6cd5\u4e0e vLLM \u7684\u57fa\u672c\u7528\u6cd5\u76f8\u540c\u3002\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u793a\u4f8b\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7 vLLM \u542f\u52a8\u4e0e OpenAI API \u517c\u5bb9\u7684\u63a5\u53e3\uff0c\u5e76\u4f7f\u7528 Qwen2-7B-Instruct-AWQ \u6a21\u578b\uff1a</p> vllm \u4f7f\u7528 AWQ \u6a21\u578b  \u5f00\u542fQwen2-7B-Instruct-AWQ\u670d\u52a1CURL\u53d1\u9001\u8bf7\u6c42Pyhton\u5ba2\u6237\u7aef\u53d1\u9001\u8bf7\u6c42 <pre><code>python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2-7B-Instruct-AWQ\n</code></pre> <pre><code>curl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"Qwen/Qwen2-7B-Instruct-AWQ\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"}\n    ],\n  }'\n</code></pre> <pre><code>from openai import OpenAI\n\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nchat_response = client.chat.completions.create(\n    model=\"Qwen/Qwen2-7B-Instruct-AWQ\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"},\n    ]\n)\nprint(\"Chat response:\", chat_response)\n</code></pre>"},{"location":"v2/awq/#3-autoawq","title":"3. \u4f7f\u7528 AutoAWQ \u91cf\u5316\u6a21\u578b","text":"<p>\u5982\u679c\u60a8\u5e0c\u671b\u5c06\u81ea\u5b9a\u4e49\u6a21\u578b\u91cf\u5316\u4e3a AWQ \u91cf\u5316\u6a21\u578b\uff0c\u6211\u4eec\u5efa\u8bae\u60a8\u4f7f\u7528 AutoAWQ\u3002\u63a8\u8350\u901a\u8fc7\u5b89\u88c5\u6e90\u4ee3\u7801\u6765\u83b7\u53d6\u5e76\u5b89\u88c5\u8be5\u5de5\u5177\u5305\u7684\u6700\u65b0\u7248\u672c\uff1a</p> <pre><code>git clone https://github.com/casper-hansen/AutoAWQ.git\ncd AutoAWQ\npip install -e .\n</code></pre> <p>\u5047\u8bbe\u4f60\u5df2\u7ecf\u57fa\u4e8e Qwen2-7B \u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u5e76\u5c06\u5176\u547d\u540d\u4e3a Qwen2-7B-finetuned\uff0c\u4e14\u4f7f\u7528\u7684\u662f\u4f60\u81ea\u5df1\u7684\u6570\u636e\u96c6\uff0c\u6bd4\u5982 Alpaca\u3002\u82e5\u8981\u6784\u5efa\u4f60\u81ea\u5df1\u7684 AWQ \u91cf\u5316\u6a21\u578b\uff0c\u4f60\u9700\u8981\u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u6821\u51c6\u3002\u4ee5\u4e0b\uff0c\u6211\u4eec\u5c06\u4e3a\u4f60\u63d0\u4f9b\u4e00\u4e2a\u7b80\u5355\u7684\u6f14\u793a\u793a\u4f8b\u4ee5\u4fbf\u8fd0\u884c\uff1a</p> <pre><code>from awq import AutoAWQForCausalLM\nfrom transformers import AutoTokenizer\n\n\nmodel_path = \"your_model_path\"\nquant_path = \"your_quantized_model_path\"\nquant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }\n\n# Load your tokenizer and model with AutoAWQ\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = AutoAWQForCausalLM.from_pretrained(\n    model_path, device_map=\"auto\", safetensors=True\n    )\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u60a8\u9700\u8981\u51c6\u5907\u6570\u636e\u4ee5\u8fdb\u884c\u6821\u51c6\u3002\u60a8\u9700\u8981\u505a\u7684\u5c31\u662f\u5c06\u6837\u672c\u653e\u5165\u4e00\u4e2a\u5217\u8868\u4e2d\uff0c\u5176\u4e2d\u6bcf\u4e2a\u6837\u672c\u90fd\u662f\u4e00\u6bb5\u6587\u672c\u3002\u7531\u4e8e\u6211\u4eec\u76f4\u63a5\u4f7f\u7528\u5fae\u8c03\u6570\u636e\u6765\u8fdb\u884c\u6821\u51c6\uff0c\u6240\u4ee5\u6211\u4eec\u9996\u5148\u4f7f\u7528 ChatML \u6a21\u677f\u5bf9\u5176\u8fdb\u884c\u683c\u5f0f\u5316\u3002</p> <pre><code>data = []\nfor msg in messages:  # (1)!\n    msg = c['messages']\n    text = tokenizer.apply_chat_template(\n              msg, tokenize=False, add_generation_prompt=False\n              )\n    data.append(text.strip())\n</code></pre> <ol> <li> <p>\u6bcf\u4e2a <code>msg</code> \u662f\u4e00\u4e2a\u5178\u578b\u7684\u804a\u5929\u6d88\u606f</p> <pre><code>[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Tell me who you are.\"},\n    {\"role\": \"assistant\", \"content\": \"I am a large language model named Qwen...\"}\n]\n</code></pre> </li> </ol> <p>\u7136\u540e\u53ea\u9700\u901a\u8fc7\u4e00\u884c\u4ee3\u7801\u8fd0\u884c\u6821\u51c6\u8fc7\u7a0b\uff1a</p> <pre><code>model.quantize(tokenizer, quant_config=quant_config, calib_data=data)\n</code></pre> <p>\u6700\u540e\uff0c\u4fdd\u5b58\u91cf\u5316\u6a21\u578b\uff1a</p> <pre><code>model.save_quantized(quant_path, safetensors=True, shard_size=\"4GB\")\ntokenizer.save_pretrained(quant_path)\n</code></pre> <p>\u8fd9\u6837\u4f60\u5c31\u53ef\u4ee5\u5f97\u5230\u4e00\u4e2a\u53ef\u4ee5\u7528\u4e8e\u90e8\u7f72\u7684 AWQ \u91cf\u5316\u6a21\u578b\u3002</p>"},{"location":"v2/chat_with_qwen2/","title":"\u4e0e Qwen2 \u8fdb\u884c\u4ea4\u4e92","text":"<p>\u4f7f\u7528 Qwen2 \u6700\u7b80\u5355\u7684\u65b9\u6cd5\u5c31\u662f\u5229\u7528 <code>transformers</code> \u5e93\u4e0e\u4e4b\u5bf9\u8bdd\u3002\u5728\u672c\u6587\u6863\u4e2d\uff0c\u6211\u4eec\u5c06\u5c55\u793a\u5982\u4f55\u5728 \u6d41\u5f0f \u6a21\u5f0f\u6216 \u975e\u6d41\u5f0f \u6a21\u5f0f\u4e0b\u4e0e Qwen2-7B-Instruct \u8fdb\u884c\u5bf9\u8bdd\u3002</p>"},{"location":"v2/chat_with_qwen2/#1","title":"1. \u57fa\u672c\u4f7f\u7528","text":"<p>\u4f60\u53ea\u9700\u501f\u52a9 <code>transformers</code> \u5e93\u7f16\u5199\u51e0\u884c\u4ee3\u7801\uff0c\u5c31\u80fd\u4e0e Qwen2-Instruct \u8fdb\u884c\u5bf9\u8bdd\u3002\u5b9e\u8d28\u4e0a\uff0c\u6211\u4eec\u901a\u8fc7 <code>from_pretrained</code> \u65b9\u6cd5\u6784\u5efa <code>tokenizer</code> \u548c\u6a21\u578b\uff0c\u7136\u540e\u5229\u7528 <code>generate</code> \u65b9\u6cd5\uff0c\u5728<code>tokenizer.apply_chat_template</code> \u7684\u8f85\u52a9\u4e0b\u8fdb\u884c\u5bf9\u8bdd\u3002</p> Qwen2-7B-Instruct \u4f7f\u7528\u793a\u4f8b \u57fa\u672c\u4f7f\u7528\u4f7f\u7528 Flash Attention2 \u52a0\u901f\u63a8\u7406 <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ndevice = \"cuda\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2-7B-Instruct\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,  # (1)!\n    tokenize=False,\n    add_generation_prompt=True  # (2)!\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512  # (3)!\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] \n    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(  # ()!\n    generated_ids, skip_special_tokens=True\n    )[0]\n</code></pre> <pre><code>1. \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u5982\u679c\u6ca1\u6709\u6307\u5b9a\u7cfb\u7edf\u63d0\u793a\uff0cQwen2 \u76f4\u63a5\u4f7f\u7528 `You are a helpful assistant.` \u4f5c\u4e3a\u7cfb\u7edf\u63d0\u793a\u3002\n2. \u7528\u4e8e\u5728\u8f93\u5165\u4e2d\u6dfb\u52a0\u751f\u6210\u63d0\u793a\uff0c\u8be5\u63d0\u793a\u6307\u5411 `&lt;|im_start|&gt;assistant\\n`\u3002\n3. `max_new_tokens` \u53c2\u6570\u7528\u4e8e\u8bbe\u7f6e\u54cd\u5e94\u7684\u6700\u5927\u957f\u5ea6\u3002\n4. \u4f7f\u7528 `tokenizer.batch_decode` \u5bf9\u54cd\u5e94\u8fdb\u884c\u89e3\u7801\u3002\n</code></pre> <pre><code>model = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2-7B-Instruct\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n)\n</code></pre> <p>model.chat \u65b9\u6cd5\u5df2\u7ecf\u5e9f\u5f03</p> <p>\u8bf7\u6ce8\u610f\uff0c\u539f Qwen \u4ed3\u5e93\u4e2d\u7684\u65e7\u65b9\u6cd5 <code>model.chat</code> \u73b0\u5728\u5df2\u88ab <code>model.generate</code> \u65b9\u6cd5\u66ff\u4ee3\u3002\u8fd9\u91cc\u4f7f\u7528\u4e86 <code>tokenizer.apply_chat_template</code> \u51fd\u6570\u5c06\u6d88\u606f\u8f6c\u6362\u4e3a\u6a21\u578b\u80fd\u591f\u7406\u89e3\u7684\u683c\u5f0f\u3002</p>"},{"location":"v2/chat_with_qwen2/#2","title":"2. \u6d41\u5f0f\u8f93\u51fa","text":"<p>\u9488\u5bf9\u6d41\u5f0f\u8f93\u51fa\u6a21\u5f0f\uff0cQwen2 \u63d0\u4f9b\u4e86 <code>TextStreamer</code> \u548c <code>TextIteratorStreamer</code> \u4e24\u79cd\u65b9\u5f0f\uff0c<code>TextStreamer</code> \u6211\u4eec\u5148\u524d\u4ecb\u7ecd\u8fc7\uff0c\u800c <code>TextIteratorStreamer</code> \u662f\u5c06\u53ef\u6253\u5370\u7684\u6587\u672c\u5b58\u50a8\u5728\u4e00\u4e2a\u961f\u5217\u4e2d\uff0c\u4ee5\u4fbf\u4e0b\u6e38\u5e94\u7528\u7a0b\u5e8f\u4f5c\u4e3a\u8fed\u4ee3\u5668\u6765\u4f7f\u7528\u3002</p> Qwen2 \u6d41\u5f0f\u4ea4\u4e92 TextStreamerTextIteratorStreamer <pre><code>from transformers import TextStreamer\n\n\nstreamer = TextStreamer(\n    tokenizer, skip_prompt=True, skip_special_tokens=True\n    )\ngenerated_ids = model.generate(\n    model_inputs,\n    max_new_tokens=512,\n    streamer=streamer,\n)\n</code></pre> <pre><code>from threading import Thread\nfrom transformers import TextIteratorStreamer\n\n\nstreamer = TextIteratorStreamer(\n    tokenizer, skip_prompt=True, skip_special_tokens=True\n    )\ngeneration_kwargs = dict(model_inputs, streamer=streamer, max_new_tokens=512)\nthread = Thread(target=model.generate, kwargs=generation_kwargs)\n\nthread.start()\ngenerated_text = \"\"\nfor new_text in streamer:\n    generated_text += new_text\n\nprint(generated_text)\n</code></pre>"},{"location":"v2/function_call/","title":"\u51fd\u6570\u8c03\u7528","text":"<p>\u5728 Qwen-Agent \u4e2d\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e13\u7528\u5c01\u88c5\u5668\uff0c\u65e8\u5728\u5b9e\u73b0\u901a\u8fc7 dashscope API \u4e0e OpenAI API \u8fdb\u884c\u7684\u51fd\u6570\u8c03\u7528\u3002</p> <pre><code>import json\nimport os\nfrom qwen_agent.llm import get_chat_model\n\n\n# Example dummy function hard coded to return the same weather\n# In production, this could be your backend API or an external API\ndef get_current_weather(location, unit='fahrenheit'):\n    \"\"\"Get the current weather in a given location\"\"\"\n    if 'tokyo' in location.lower():\n        return json.dumps({\n            'location': 'Tokyo',\n            'temperature': '10',\n            'unit': 'celsius'\n        })\n    elif 'san francisco' in location.lower():\n        return json.dumps({\n            'location': 'San Francisco',\n            'temperature': '72',\n            'unit': 'fahrenheit'\n        })\n    elif 'paris' in location.lower():\n        return json.dumps({\n            'location': 'Paris',\n            'temperature': '22',\n            'unit': 'celsius'\n        })\n    else:\n        return json.dumps({'location': location, 'temperature': 'unknown'})\n\n\ndef test():\n    llm = get_chat_model({\n        # Use the model service provided by DashScope:\n        'model': 'qwen-max',\n        'model_server': 'dashscope',\n        'api_key': os.getenv('DASHSCOPE_API_KEY'),\n\n        # Use the model service provided by Together.AI:\n        # 'model': 'Qwen/Qwen2-7B-Instruct',\n        # 'model_server': 'https://api.together.xyz',  # api_base\n        # 'api_key': os.getenv('TOGETHER_API_KEY'),\n\n        # Use your own model service compatible with OpenAI API:\n        # 'model': 'Qwen/Qwen2-72B-Instruct',\n        # 'model_server': 'http://localhost:8000/v1',  # api_base\n        # 'api_key': 'EMPTY',\n    })\n\n    # Step 1: send the conversation and available functions to the model\n    messages = [{\n        'role': 'user',\n        'content': \"What's the weather like in San Francisco?\"\n    }]\n    functions = [{\n        'name': 'get_current_weather',\n        'description': 'Get the current weather in a given location',\n        'parameters': {\n            'type': 'object',\n            'properties': {\n                'location': {\n                    'type': 'string',\n                    'description':\n                    'The city and state, e.g. San Francisco, CA',\n                },\n                'unit': {\n                    'type': 'string',\n                    'enum': ['celsius', 'fahrenheit']\n                },\n            },\n            'required': ['location'],\n        },\n    }]\n\n    print('# Assistant Response 1:')\n    responses = []\n    for responses in llm.chat(messages=messages,\n                              functions=functions,\n                              stream=True):\n        print(responses)\n\n    messages.extend(responses)  # extend conversation with assistant's reply\n\n    # Step 2: check if the model wanted to call a function\n    last_response = messages[-1]\n    if last_response.get('function_call', None):\n\n        # Step 3: call the function\n        # Note: the JSON response may not always be valid; be sure to handle errors\n        available_functions = {\n            'get_current_weather': get_current_weather,\n        }  # only one function in this example, but you can have multiple\n        function_name = last_response['function_call']['name']\n        function_to_call = available_functions[function_name]\n        function_args = json.loads(last_response['function_call']['arguments'])\n        function_response = function_to_call(\n            location=function_args.get('location'),\n            unit=function_args.get('unit'),\n        )\n        print('# Function Response:')\n        print(function_response)\n\n        # Step 4: send the info for each function call and function response to the model\n        messages.append({\n            'role': 'function',\n            'name': function_name,\n            'content': function_response,\n        })  # extend conversation with function response\n\n        print('# Assistant Response 2:')\n        for responses in llm.chat(\n                messages=messages,\n                functions=functions,\n                stream=True,\n        ):  # get a new response from the model where it can see the function response\n            print(responses)\n\n\nif __name__ == '__main__':\n    test()\n</code></pre>"},{"location":"v2/gguf/","title":"GGUF","text":"<p>\u6700\u8fd1\uff0c\u5728\u793e\u533a\u4e2d\u672c\u5730\u8fd0\u884c LLM \u53d8\u5f97\u8d8a\u6765\u8d8a\u6d41\u884c\uff0c\u5176\u4e2d\u4f7f\u7528 llama.cpp \u5904\u7406 GGUF \u6587\u4ef6\u662f\u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\u3002\u901a\u8fc7 llama.cpp\uff0c\u60a8\u4e0d\u4ec5\u53ef\u4ee5\u4e3a\u81ea\u5df1\u7684\u6a21\u578b\u6784\u5efa GGUF \u6587\u4ef6\uff0c\u8fd8\u53ef\u4ee5\u8fdb\u884c\u4f4e\u6bd4\u7279\u91cf\u5316\u64cd\u4f5c\u3002\u5728 GGUF \u683c\u5f0f\u4e0b\uff0c\u60a8\u53ef\u4ee5\u76f4\u63a5\u5bf9\u6a21\u578b\u8fdb\u884c\u91cf\u5316\u800c\u65e0\u9700\u6821\u51c6\u8fc7\u7a0b\uff0c\u6216\u8005\u4e3a\u4e86\u83b7\u5f97\u66f4\u597d\u7684\u91cf\u5316\u6548\u679c\u5e94\u7528 AWQ scale\uff0c\u4ea6\u6216\u662f\u7ed3\u5408\u6821\u51c6\u6570\u636e\u4f7f\u7528 imatrix \u5de5\u5177\u3002\u5728\u8fd9\u7bc7\u6587\u6863\u4e2d\uff0c\u6211\u4eec\u5c06\u5c55\u793a\u6700\u7b80\u4fbf\u7684\u6a21\u578b\u91cf\u5316\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5982\u4f55\u5728\u5bf9 Qwen \u6a21\u578b\u8fdb\u884c\u91cf\u5316\u65f6\u5e94\u7528 AWQ \u6bd4\u4f8b\u4ee5\u4f18\u5316\u5176\u8d28\u91cf\u3002</p>"},{"location":"v2/gguf/#1-gguf","title":"1. \u751f\u6210\u91cf\u5316\u7684 GGUF \u6a21\u578b\u6587\u4ef6","text":"<p>\u5728\u8fdb\u884c\u91cf\u5316\u64cd\u4f5c\u4e4b\u524d\uff0c\u8bf7\u786e\u4fdd\u4f60\u5df2\u7ecf\u6309\u7167\u6307\u5bfc\u5f00\u59cb\u4f7f\u7528 llama.cpp\u3002\u4ee5\u4e0b\u6307\u5f15\u5c06\u4e0d\u4f1a\u63d0\u4f9b\u6709\u5173\u5b89\u88c5\u548c\u6784\u5efa\u7684\u6b65\u9aa4\u3002\u73b0\u5728\uff0c\u5047\u8bbe\u4f60\u8981\u5bf9 Qwen2-7B-Instruct \u6a21\u578b\u8fdb\u884c\u91cf\u5316\uff0c\u9996\u5148\u9700\u8981\u6309\u7167\u5982\u4e0b\u6240\u793a\u7684\u65b9\u5f0f\u4e3a fp16 \u6a21\u578b\u521b\u5efa\u4e00\u4e2a GGUF \u6587\u4ef6\uff1a</p> <pre><code>python convert-hf-to-gguf.py Qwen/Qwen2-7B-Instruct \\  # (1)!\n    --outfile models/7B/qwen2-7b-instruct-fp16.gguf  # (2)!\n</code></pre> <ol> <li>\u9884\u8bad\u7ec3\u6a21\u578b\u6240\u5728\u7684\u8def\u5f84\u6216\u8005 HF \u6a21\u578b\u7684\u540d\u79f0</li> <li>\u6240\u751f\u6210\u7684 GGUF \u6587\u4ef6\u7684\u8def\u5f84</li> </ol> <p>\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u4f60\u5df2\u7ecf\u4e3a\u4f60\u7684 fp16 \u6a21\u578b\u751f\u6210\u4e86\u4e00\u4e2a GGUF \u6587\u4ef6\uff0c\u63a5\u4e0b\u6765\u4f60\u9700\u8981\u6839\u636e\u5b9e\u9645\u9700\u6c42\u5c06\u5176\u91cf\u5316\u81f3\u4f4e\u6bd4\u7279\u4f4d\u3002</p> <pre><code>./llama-quantize models/7B/qwen2-7b-instruct-fp16.gguf models/7B/qwen2-7b-instruct-q4_0.gguf q4_0\n</code></pre> <p>\u6267\u884c\u5b8c\u6210\u540e\uff0c\u6211\u4eec\u5c31\u5c06\u5c06\u6a21\u578b\u91cf\u5316\u4e3a 4 \u6bd4\u7279\u7684 GGUF \u6587\u4ef6\uff0c\u5176\u4e2d <code>q4_0</code> \u8868\u793a 4 \u6bd4\u7279\u91cf\u5316\u3002\u73b0\u5728\uff0c\u8fd9\u4e2a\u91cf\u5316\u540e\u7684\u6a21\u578b\u53ef\u4ee5\u76f4\u63a5\u901a\u8fc7 llama.cpp \u8fd0\u884c\u3002</p>"},{"location":"v2/gguf/#2-awq-scales","title":"2. \u4f7f\u7528 AWQ scales \u91cf\u5316\u6a21\u578b","text":"<p>\u8981\u63d0\u5347\u91cf\u5316\u6a21\u578b\u7684\u8d28\u91cf\uff0c\u4e00\u79cd\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u662f\u5e94\u7528 AWQ scales\uff0c\u53ef\u4ee5\u53c2\u8003\u8be5\u811a\u672c\u3002</p> <p>\u5177\u4f53\u64cd\u4f5c\u6b65\u9aa4\u5982\u4e0b\uff1a\u9996\u5148\u5728\u4f7f\u7528 AutoAWQ \u8fd0\u884c <code>model.quantize()</code> \u65f6\uff0c\u8bf7\u52a1\u5fc5\u8bb0\u5f97\u6dfb\u52a0 <code>export_compatible=True</code> \u53c2\u6570\uff0c\u5982\u4e0b\u6240\u793a\uff1a</p> <pre><code>model.quantize(\n    tokenizer,\n    quant_config=quant_config,\n    export_compatible=True\n)\nmodel.save_pretrained(quant_path)\n</code></pre> <p>\u901a\u8fc7 <code>model.save_quantized()</code> \u65b9\u6cd5\u4fdd\u5b58 AWQ scales \u7684 FP16 \u6a21\u578b\u3002\u7136\u540e\uff0c\u5f53\u4f60\u8fd0\u884c convert-hf-to-gguf.py \u811a\u672c\u65f6\uff0c\u8bb0\u5f97\u5c06\u6a21\u578b\u8def\u5f84\u66ff\u6362\u4e3a\u5e26\u6709 AWQ scales \u7684 FP16 \u6a21\u578b\u7684\u8def\u5f84\u3002</p> <p>```bash linenum=\"1\" python convert-hf-to-gguf.py ${quant_path} \\   --outfile models/7B/qwen2-7b-instruct-fp16-awq.gguf <pre><code>\u901a\u8fc7\u8fd9\u79cd\u65b9\u5f0f\uff0c\u60a8\u53ef\u4ee5\u5728 GGUF \u683c\u5f0f\u7684\u91cf\u5316\u6a21\u578b\u4e2d\u5e94\u7528 AWQ scales\uff0c\u8fd9\u6709\u52a9\u4e8e\u63d0\u5347\u6a21\u578b\u7684\u8d28\u91cf\u3002\n\n\u6211\u4eec\u901a\u5e38\u53ef\u4ee5\u5c06 FP16 \u6a21\u578b\u91cf\u5316\u4e3a2\u30013\u30014\u30015\u30016\u30018 \u4f4d\u6a21\u578b\u3002\u8981\u6267\u884c\u4e0d\u540c\u4f4e\u6bd4\u7279\u7684\u91cf\u5316\uff0c\u53ea\u9700\u5728\u547d\u4ee4\u4e2d\u66ff\u6362\u91cf\u5316\u65b9\u6cd5\u5373\u53ef\u3002\u4f8b\u5982\uff0c\u5982\u679c\u4f60\u60f3\u5c06\u4f60\u7684\u6a21\u578b\u91cf\u5316\u4e3a 2 \u4f4d\u6a21\u578b\uff0c\u4f60\u53ef\u4ee5\u6309\u7167\u4e0b\u9762\u6240\u793a\uff0c\u5c06 `q4_0` \u66ff\u6362\u4e3a `q2_k`\u3002\n\n```bash linenums=\"1\"\n./llama-quantize models/7B/qwen2-7b-instruct-fp16.gguf models/7B/qwen2-7b-instruct-q2_k.gguf q2_k\n</code></pre></p> <p>\u76ee\u524d\u652f\u6301\u7684\u91cf\u5316\u7ea7\u522b GGUF \u6a21\u578b\uff1a<code>q2_k</code>\u3001<code>q3_k_m</code>\u3001<code>q4_0</code>\u3001<code>q4_k_m</code>\u3001<code>q5_0</code>\u3001<code>q5_k_m</code>\u3001 <code>q6_k</code> \u548c <code>q8_0</code>\u3002</p>"},{"location":"v2/gptq/","title":"GPTQ","text":"<p>GPTQ \u662f\u4e00\u79cd\u9488\u5bf9\u7c7b GPT \u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u5b83\u57fa\u4e8e\u8fd1\u4f3c \u4e8c\u9636\u4fe1\u606f \u8fdb\u884c\u4e00\u6b21\u6027\u6743\u91cd\u91cf\u5316\u3002\u5728\u672c\u6587\u6863\u4e2d\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e0b\u5982\u4f55\u4f7f\u7528 <code>transformers</code> \u5e93\u52a0\u8f7d\u5e76\u5e94\u7528\u91cf\u5316\u540e\u7684\u6a21\u578b\uff0c\u4ee5\u53ca\u5982\u4f55\u901a\u8fc7 AutoGPTQ \u6765\u5bf9\u60a8\u81ea\u5df1\u7684\u6a21\u578b\u8fdb\u884c\u91cf\u5316\u5904\u7406\u3002</p>"},{"location":"v2/gptq/#1-transformers-gptq","title":"1. \u5728 Transformers \u4e2d\u4f7f\u7528 GPTQ \u6a21\u578b","text":"<p>\u73b0\u5728\uff0cTransformers \u6b63\u5f0f\u652f\u6301\u4e86 AutoGPTQ\uff0c\u8fd9\u610f\u5473\u7740\u6211\u4eec\u80fd\u591f\u76f4\u63a5\u5728 Transformers \u4e2d\u4f7f\u7528\u91cf\u5316\u540e\u7684\u6a21\u578b\u3002</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ndevice = \"cuda\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2-7B-Instruct-GPTQ-Int8\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct-GPTQ-Int8\")\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512\n)\ngenerated_ids = [\n    output_ids[len(input_ids):] \n    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(\n        generated_ids, skip_special_tokens=True\n        )[0]\n</code></pre>"},{"location":"v2/gptq/#2-vllm-gptq","title":"2. \u5728 vLLM \u4e2d\u4f7f\u7528 GPTQ \u6a21\u578b","text":"<p>vLLM \u5df2\u7ecf\u652f\u6301\u4e86 GPTQ\uff0c\u8fd9\u610f\u5473\u7740\u6211\u4eec\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 GPTQ \u6a21\u578b\uff0c\u6216\u8005\u5c06 AutoGPTQ \u8bad\u7ec3\u5f97\u5230\u7684\u6a21\u578b\u4e0e vLLM \u7ed3\u5408\u4f7f\u7528\u3002\u5b9e\u9645\u4e0a\uff0c\u5176\u7528\u6cd5\u4e0e vLLM \u7684\u57fa\u672c\u7528\u6cd5\u76f8\u540c\u3002</p> \u4f7f\u7528 GPTQ \u6a21\u578b \u8fd0\u884c vLLM \u670d\u52a1CURL\u53d1\u9001\u8bf7\u6c42Python\u53d1\u9001\u8bf7\u6c42 <pre><code>python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2-7B-Instruct-GPTQ-Int8\n</code></pre> <pre><code>curl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\ \n  -d '{\n    \"model\": \"Qwen/Qwen2-7B-Instruct-GPTQ-Int8\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"}\n    ],\n  }'\n</code></pre> <pre><code>from openai import OpenAI\n\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nchat_response = client.chat.completions.create(\n    model=\"Qwen/Qwen2-7B-Instruct-GPTQ-Int8\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"},\n    ]\n)\nprint(\"Chat response:\", chat_response)\n</code></pre>"},{"location":"v2/gptq/#3-autogptq","title":"3. \u4f7f\u7528 AutoGPTQ \u91cf\u5316\u4f60\u7684\u6a21\u578b","text":"<p>\u5982\u679c\u4f60\u60f3\u5c06\u5fae\u8c03\u540e\u7684\u6a21\u578b\u91cf\u5316\u4e3a GPTQ \u6a21\u578b\uff0c\u6211\u4eec\u5efa\u8bae\u4f60\u4f7f\u7528 AutoGPTQ \u5de5\u5177\u3002\u63a8\u8350\u901a\u8fc7\u5b89\u88c5\u6e90\u4ee3\u7801\u7684\u65b9\u5f0f\u83b7\u53d6\u5e76\u5b89\u88c5\u6700\u65b0\u7248\u672c\u7684\u8be5\u8f6f\u4ef6\u5305\u3002</p> <pre><code>git clone https://github.com/AutoGPTQ/AutoGPTQ\ncd AutoGPTQ\npip install -e .\n</code></pre> <p>\u5047\u8bbe\u4f60\u5df2\u7ecf\u57fa\u4e8e Qwen2-7B \u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u5e76\u5c06\u8be5\u5fae\u8c03\u540e\u7684\u6a21\u578b\u547d\u540d\u4e3a Qwen2-7B-finetuned \uff0c\u4e14\u4f7f\u7528\u7684\u662f\u81ea\u5df1\u7684\u6570\u636e\u96c6\uff0c\u6bd4\u5982 Alpaca\u3002\u8981\u6784\u5efa\u4f60\u81ea\u5df1\u7684 GPTQ \u91cf\u5316\u6a21\u578b\uff0c\u4f60\u9700\u8981\u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u6821\u51c6\u3002</p> <pre><code>from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nfrom transformers import AutoTokenizer\n\nmodel_path = \"your_model_path\"\nquant_path = \"your_quantized_model_path\"\nquantize_config = BaseQuantizeConfig(\n    bits=8, # (1)!\n    group_size=128,\n    damp_percent=0.01,\n    desc_act=False,  # (2)!\n    static_groups=False,\n    sym=True,\n    true_sequential=True,\n    model_name_or_path=None,\n    model_file_base_name=\"model\"\n)\nmax_len = 8192\ntokenizer = AutoTokenizer.from_pretrained(model_path)  # (3)!\nmodel = AutoGPTQForCausalLM.from_pretrained(model_path, quantize_config)\n</code></pre> <ol> <li>\u53ef\u8bbe\u7f6e\u4e3a 4 \u6216 8 \u4f4d</li> <li>\u8bbe\u7f6e\u4e3a <code>False</code> \u53ef\u4ee5\u660e\u663e\u52a0\u901f\u63a8\u7406\uff0c\u4f46\u56f0\u60d1\u5ea6\u53ef\u80fd\u6709\u70b9\u5dee</li> <li>\u5982\u679c\u60f3\u8981\u4e86\u89e3\u4f7f\u7528\u591a GPU \u52a0\u8f7d\u6a21\u578b\uff0c\u8bf7\u53c2\u9605 AutoGPTQ\u6587\u6863\u3002</li> </ol> <p>\u4f46\u662f\uff0c\u5982\u679c\u4f60\u60f3\u4f7f\u7528\u591a GPU \u6765\u8bfb\u53d6\u6a21\u578b\uff0c\u4f60\u9700\u8981\u4f7f\u7528 <code>max_memory</code> \u800c\u4e0d\u662f <code>device_map</code>\u3002</p> <pre><code>model = AutoGPTQForCausalLM.from_pretrained(\n    model_path,\n    quantize_config,\n    max_memory={i:\"20GB\" for i in range(4)}\n)\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u4f60\u9700\u8981\u51c6\u5907\u6570\u636e\u8fdb\u884c\u6821\u51c6\u3002\u4f60\u9700\u8981\u505a\u7684\u662f\u5c06\u6837\u672c\u653e\u5165\u4e00\u4e2a\u5217\u8868\u4e2d\uff0c\u5176\u4e2d\u6bcf\u4e2a\u6837\u672c\u90fd\u662f\u4e00\u6bb5\u6587\u672c\u3002\u7531\u4e8e\u6211\u4eec\u76f4\u63a5\u4f7f\u7528\u5fae\u8c03\u6570\u636e\u8fdb\u884c\u6821\u51c6\uff0c\u6240\u4ee5\u6211\u4eec\u9996\u5148\u4f7f\u7528 ChatML \u6a21\u677f\u5bf9\u5b83\u8fdb\u884c\u683c\u5f0f\u5316\u5904\u7406\u3002</p> <pre><code>import torch\n\ndata = []\nfor msg in messages:  # (1) !\n    text = tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=False)\n    model_inputs = tokenizer([text])\n    input_ids = torch.tensor(model_inputs.input_ids[:max_len], dtype=torch.int)\n    data.append(dict(\n        input_ids=input_ids, \n        attention_mask=input_ids.ne(tokenizer.pad_token_id)\n        ))\n</code></pre> <ol> <li> <p><code>msg</code> \u7684\u5185\u5bb9\u5982\u4e0b:</p> <pre><code>[\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Tell me who you are.\"},\n    {\"role\": \"assistant\", \"content\": \"I am a large language model named Qwen...\"}\n]\n</code></pre> </li> </ol> <p>\u7136\u540e\u53ea\u9700\u901a\u8fc7\u4e00\u884c\u4ee3\u7801\u8fd0\u884c\u6821\u51c6\u8fc7\u7a0b\uff1a</p> <pre><code>model.quantize(data, cache_examples_on_gpu=False)\n</code></pre> <p>\u6700\u540e\uff0c\u4fdd\u5b58\u91cf\u5316\u6a21\u578b\uff1a</p> <pre><code>model.save_quantized(quant_path, use_safetensors=True)\ntokenizer.save_pretrained(quant_path)\n</code></pre> <p>\u4e0d\u652f\u6301\u5207\u7247</p> <p><code>save_quantized</code> \u65b9\u6cd5\u4e0d\u652f\u6301\u6a21\u578b\u5206\u7247\u3002\u82e5\u8981\u5b9e\u73b0\u6a21\u578b\u5206\u7247\uff0c\u6211\u4eec\u9700\u8981\u5148\u52a0\u8f7d\u6a21\u578b\uff0c\u7136\u540e\u4f7f\u7528\u6765\u81ea transformers \u5e93\u7684 <code>save_pretrained</code> \u65b9\u6cd5\u6765\u4fdd\u5b58\u5e76\u5206\u7247\u6a21\u578b\u3002</p>"},{"location":"v2/installation/","title":"\u5b89\u88c5","text":"<p>\u4f7f\u7528 Qwen2 \u9700\u8981\u5b89\u88c5 HuggingFace \u7684 <code>transformers</code> \u5e93\uff08\u5efa\u8bae\u4f7f\u7528\u6700\u65b0\u7248\u672c\uff0c\u81f3\u5c11\u662f 4.40.0 \u7248\u672c\u4ee5\u4e0a\uff09\uff0c\u5b89\u88c5\u5b8c\u6210\u4e4b\u540e\u5c31\u53ef\u4ee5\u4f7f\u7528 Qwen2 \u96c6\u5408\u4e2d\u7684\u6a21\u578b\u4e86\u3002</p> <p>Transformers \u7684\u4e09\u79cd\u5b89\u88c5\u65b9\u5f0f</p> PIPCONDA\u6e90\u7801\u5b89\u88c5 <pre><code>pip install transformers -U\n</code></pre> <pre><code>conda install conda-forge::transformers\n</code></pre> <pre><code>pip install git+https://github.com/huggingface/transformers\n</code></pre> <p>\u5efa\u8bae\u521b\u5efa\u65b0\u7684\u865a\u62df\u73af\u5883\uff0c\u518d\u901a\u8fc7 pip \u6216 conda \u8fdb\u884c\u5b89\u88c5\u3002\u6b64\u5916\uff0c\u5efa\u8bae\u4f7f\u7528 Python 3.8 \u548c Pytorch 2.0 \u4ee5\u4e0a\u7248\u672c\u3002</p> <p>\u8fd9\u91cc\u53ea\u662f\u5b89\u88c5\u4e86 <code>transformers</code> \u5e93\uff0c\u5b9e\u9645\u4e0a\u4f7f\u7528 Qwen \u8fd8\u9700\u8981\u5b89\u88c5\u66f4\u591a\u4f9d\u8d56\u5e93\u3002</p>"},{"location":"v2/langchain/","title":"LangChain","text":"<p>\u672c\u6559\u7a0b\u65e8\u5728\u5e2e\u52a9\u60a8\u5229\u7528 <code>Qwen2-7B-Instruct</code> \u4e0e <code>langchain</code>\uff0c\u57fa\u4e8e\u672c\u5730\u77e5\u8bc6\u5e93\u6784\u5efa\u95ee\u7b54\u5e94\u7528\u3002\u76ee\u6807\u662f\u5efa\u7acb\u4e00\u4e2a\u77e5\u8bc6\u5e93\u95ee\u7b54\u89e3\u51b3\u65b9\u6848\u3002</p>"},{"location":"v2/langchain/#1","title":"1. \u5b89\u88c5","text":"<pre><code>pip install langchain==0.0.174\npip install faiss-gpu\n</code></pre>"},{"location":"v2/langchain/#2","title":"2. \u57fa\u7840\u7528\u6cd5","text":"<p>\u60a8\u53ef\u4ee5\u4ec5\u4f7f\u7528\u60a8\u7684\u6587\u6863\u914d\u5408 <code>langchain</code> \u6765\u6784\u5efa\u4e00\u4e2a\u95ee\u7b54\u5e94\u7528\u3002\u8be5\u9879\u76ee\u7684\u5b9e\u73b0\u6d41\u7a0b\u5305\u62ec\u52a0\u8f7d\u6587\u4ef6 \u2192 \u9605\u8bfb\u6587\u672c \u2192 \u6587\u672c\u5206\u6bb5 \u2192 \u6587\u672c\u5411\u91cf\u5316 \u2192 \u95ee\u9898\u5411\u91cf\u5316 \u2192 \u5c06\u6700\u76f8\u4f3c\u7684\u524d k \u4e2a\u6587\u672c\u5411\u91cf\u4e0e\u95ee\u9898\u5411\u91cf\u5339\u914d \u2192 \u5c06\u5339\u914d\u7684\u6587\u672c\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u8fde\u540c\u95ee\u9898\u4e00\u8d77\u7eb3\u5165\u63d0\u793a \u2192 \u63d0\u4ea4\u7ed9 Qwen2-7B-Instruct \u751f\u6210\u7b54\u6848\u3002</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer\nfrom abc import ABC\nfrom langchain.llms.base import LLM\nfrom typing import Any, List, Mapping, Optional\nfrom langchain.callbacks.manager import CallbackManagerForLLMRun\ndevice = \"cuda\" # the device to load the model onto\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2-7B-Instruct\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")\n\nclass Qwen(LLM, ABC):\n     max_token: int = 10000\n     temperature: float = 0.01\n     top_p = 0.9\n     history_len: int = 3\n\n     def __init__(self):\n         super().__init__()\n\n     @property\n     def _llm_type(self) -&gt; str:\n         return \"Qwen\"\n\n     @property\n     def _history_len(self) -&gt; int:\n         return self.history_len\n\n     def set_history_len(self, history_len: int = 10) -&gt; None:\n         self.history_len = history_len\n\n     def _call(\n         self,\n         prompt: str,\n         stop: Optional[List[str]] = None,\n         run_manager: Optional[CallbackManagerForLLMRun] = None,\n     ) -&gt; str:\n         messages = [\n             {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n             {\"role\": \"user\", \"content\": prompt}\n         ]\n         text = tokenizer.apply_chat_template(\n             messages,\n             tokenize=False,\n             add_generation_prompt=True\n         )\n         model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n         generated_ids = model.generate(\n             model_inputs.input_ids,\n             max_new_tokens=512\n         )\n         generated_ids = [\n             output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n         ]\n\n         response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n         return response\n\n     @property\n     def _identifying_params(self) -&gt; Mapping[str, Any]:\n         \"\"\"Get the identifying parameters.\"\"\"\n         return {\"max_token\": self.max_token,\n                 \"temperature\": self.temperature,\n                 \"top_p\": self.top_p,\n                 \"history_len\": self.history_len}\n</code></pre> <p>\u52a0\u8f7d Qwen2-7B-Instruct \u6a21\u578b\u540e\uff0c\u60a8\u53ef\u4ee5\u6307\u5b9a\u9700\u8981\u7528\u4e8e\u77e5\u8bc6\u5e93\u95ee\u7b54\u7684 txt \u6587\u4ef6\u3002</p> <p>``import os import re import torch import argparse from langchain.vectorstores import FAISS from langchain.embeddings.huggingface import HuggingFaceEmbeddings from typing import List, Tuple import numpy as np from langchain.document_loaders import TextLoader from langchain.text_splitter import CharacterTextSplitter from langchain.docstore.document import Document from langchain.prompts.prompt import PromptTemplate from langchain.chains import RetrievalQA</p> <p>class ChineseTextSplitter(CharacterTextSplitter):     def init(self, pdf: bool = False, **kwargs):         super().init(**kwargs)         self.pdf = pdf</p> <pre><code>def split_text(self, text: str) -&gt; List[str]:\n    if self.pdf:\n        text = re.sub(r\"\\n{3,}\", \"\\n\", text)\n        text = re.sub('\\s', ' ', text)\n        text = text.replace(\"\\n\\n\", \"\")\n    sent_sep_pattern = re.compile(\n        '([\ufe52\ufe54\ufe56\ufe57\uff0e\u3002\uff01\uff1f][\"\u2019\u201d\u300d\u300f]{0,2}|(?=[\"\u2018\u201c\u300c\u300e]{1,2}|$))')\n    sent_list = []\n    for ele in sent_sep_pattern.split(text):\n        if sent_sep_pattern.match(ele) and sent_list:\n            sent_list[-1] += ele\n        elif ele:\n            sent_list.append(ele)\n    return sent_list\n</code></pre> <p>def load_file(filepath):     loader = TextLoader(filepath, autodetect_encoding=True)     textsplitter = ChineseTextSplitter(pdf=False)     docs = loader.load_and_split(textsplitter)     write_check_file(filepath, docs)     return docs</p> <p>def write_check_file(filepath, docs):     folder_path = os.path.join(os.path.dirname(filepath), \"tmp_files\")     if not os.path.exists(folder_path):         os.makedirs(folder_path)     fp = os.path.join(folder_path, 'load_file.txt')     with open(fp, 'a+', encoding='utf-8') as fout:         fout.write(\"filepath=%s,len=%s\" % (filepath, len(docs)))         fout.write('\\n')         for i in docs:             fout.write(str(i))             fout.write('\\n')         fout.close()</p> <p>def separate_list(ls: List[int]) -&gt; List[List[int]]:     lists = []     ls1 = [ls[0]]     for i in range(1, len(ls)):         if ls[i - 1] + 1 == ls[i]:             ls1.append(ls[i])         else:             lists.append(ls1)             ls1 = [ls[i]]     lists.append(ls1)     return lists</p> <p>class FAISSWrapper(FAISS):     chunk_size = 250     chunk_conent = True     score_threshold = 0</p> <pre><code>def similarity_search_with_score_by_vector(\n        self, embedding: List[float], k: int = 4\n) -&gt; List[Tuple[Document, float]]:\n    scores, indices = self.index.search(np.array([embedding], dtype=np.float32), k)\n    docs = []\n    id_set = set()\n    store_len = len(self.index_to_docstore_id)\n    for j, i in enumerate(indices[0]):\n        if i == -1 or 0 &lt; self.score_threshold &lt; scores[0][j]:\n            # This happens when not enough docs are returned.\n            continue\n        _id = self.index_to_docstore_id[i]\n        doc = self.docstore.search(_id)\n        if not self.chunk_conent:\n            if not isinstance(doc, Document):\n                raise ValueError(f\"Could not find document for id {_id}, got {doc}\")\n            doc.metadata[\"score\"] = int(scores[0][j])\n            docs.append(doc)\n            continue\n        id_set.add(i)\n        docs_len = len(doc.page_content)\n        for k in range(1, max(i, store_len - i)):\n            break_flag = False\n            for l in [i + k, i - k]:\n                if 0 &lt;= l &lt; len(self.index_to_docstore_id):\n                    _id0 = self.index_to_docstore_id[l]\n                    doc0 = self.docstore.search(_id0)\n                    if docs_len + len(doc0.page_content) &gt; self.chunk_size:\n                        break_flag = True\n                        break\n                    elif doc0.metadata[\"source\"] == doc.metadata[\"source\"]:\n                        docs_len += len(doc0.page_content)\n                        id_set.add(l)\n            if break_flag:\n                break\n    if not self.chunk_conent:\n        return docs\n    if len(id_set) == 0 and self.score_threshold &gt; 0:\n        return []\n    id_list = sorted(list(id_set))\n    id_lists = separate_list(id_list)\n    for id_seq in id_lists:\n        for id in id_seq:\n            if id == id_seq[0]:\n                _id = self.index_to_docstore_id[id]\n                doc = self.docstore.search(_id)\n            else:\n                _id0 = self.index_to_docstore_id[id]\n                doc0 = self.docstore.search(_id0)\n                doc.page_content += \" \" + doc0.page_content\n        if not isinstance(doc, Document):\n            raise ValueError(f\"Could not find document for id {_id}, got {doc}\")\n        doc_score = min([scores[0][id] for id in [indices[0].tolist().index(i) for i in id_seq if i in indices[0]]])\n        doc.metadata[\"score\"] = int(doc_score)\n        docs.append((doc, doc_score))\n    return docs\n</code></pre> <p>if name == 'main':     # load docs (pdf file or txt file)     filepath = 'your file path'     # Embedding model name     EMBEDDING_MODEL = 'text2vec'     PROMPT_TEMPLATE = \"\"\"Known information:     {context_str}     Based on the above known information, respond to the user's question concisely and professionally. If an answer cannot be derived from it, say 'The question cannot be answered with the given information' or 'Not enough relevant information has been provided,' and do not include fabricated details in the answer. Please respond in English. The question is {question}\"\"\"     # Embedding running device     EMBEDDING_DEVICE = \"cuda\"     # return top-k text chunk from vector store     VECTOR_SEARCH_TOP_K = 3     CHAIN_TYPE = 'stuff'     embedding_model_dict = {         \"text2vec\": \"your text2vec model path\",     }     llm = Qwen()     embeddings = HuggingFaceEmbeddings(model_name=embedding_model_dict[EMBEDDING_MODEL],model_kwargs={'device': EMBEDDING_DEVICE})</p> <pre><code>docs = load_file(filepath)\n\ndocsearch = FAISSWrapper.from_documents(docs, embeddings)\n\nprompt = PromptTemplate(\n    template=PROMPT_TEMPLATE, input_variables=[\"context_str\", \"question\"]\n)\n\nchain_type_kwargs = {\"prompt\": prompt, \"document_variable_name\": \"context_str\"}\nqa = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=CHAIN_TYPE,\n    retriever=docsearch.as_retriever(search_kwargs={\"k\": VECTOR_SEARCH_TOP_K}),\n    chain_type_kwargs=chain_type_kwargs)\n\nquery = \"Give me a short introduction to large language model.\"\nprint(qa.run(query))\n</code></pre> <p>```</p>"},{"location":"v2/llama.cpp/","title":"llama.cpp","text":"<p>llama.cpp \u662f\u4e00\u4e2a C++ \u5e93\uff0c\u7528\u4e8e\u7b80\u5316 LLM \u63a8\u7406\u7684\u8bbe\u7f6e\uff0c\u5b83\u4f7f\u5f97\u5728\u672c\u5730\u673a\u5668\u4e0a\u8fd0\u884c Qwen \u6210\u4e3a\u53ef\u80fd\u3002\u8be5\u5e93\u662f\u4e00\u4e2a\u7eaf C/C++ \u5b9e\u73b0\uff0c\u4e0d\u4f9d\u8d56\u4efb\u4f55\u5916\u90e8\u5e93\uff0c\u5e76\u4e14\u9488\u5bf9 x86 \u67b6\u6784\u63d0\u4f9b\u4e86 AVX\u3001AVX2 \u548c AVX512 \u52a0\u901f\u652f\u6301\u3002</p> <p>\u6b64\u5916\uff0c\u5b83\u8fd8\u63d0\u4f9b\u4e86 2\u30013\u30014\u30015\u30016 \u4ee5\u53ca 8 \u4f4d\u91cf\u5316\u529f\u80fd\uff0c\u4ee5\u52a0\u5feb\u63a8\u7406\u901f\u5ea6\u5e76\u51cf\u5c11\u5185\u5b58\u5360\u7528\u3002\u5bf9\u4e8e\u5927\u4e8e\u603b VRAM \u5bb9\u91cf\u7684\u5927\u89c4\u6a21\u6a21\u578b\uff0c\u8be5\u5e93\u8fd8\u652f\u6301 CPU+GPU \u6df7\u5408\u63a8\u7406\u6a21\u5f0f\u8fdb\u884c\u90e8\u5206\u52a0\u901f\u3002\u672c\u8d28\u4e0a\uff0cllama.cpp \u7684\u7528\u9014\u5728\u4e8e\u8fd0\u884c GGUF\uff08\u7531 GPT \u751f\u6210\u7684\u7edf\u4e00\u683c\u5f0f\uff09\u6a21\u578b\u3002</p>"},{"location":"v2/llama.cpp/#1","title":"1. \u51c6\u5907","text":"<p>\u8fd9\u4e00\u64cd\u4f5c\u9002\u7528\u4e8e Linux \u6216\u8005 MacOS \u7cfb\u7edf\u3002</p> <pre><code>git clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\nmake\n</code></pre>"},{"location":"v2/llama.cpp/#2-gguf","title":"2.\u8fd0\u884c GGUF \u6587\u4ef6","text":"<p>\u6211\u4eec\u5728 Hugging Face \u7ec4\u7ec7\u4e2d\u63d0\u4f9b\u4e86\u4e00\u7cfb\u5217 GGUF \u6a21\u578b\uff0c\u4e3a\u4e86\u627e\u5230\u60a8\u9700\u8981\u7684\u6a21\u578b\uff0c\u60a8\u53ef\u4ee5\u641c\u7d22\u4ed3\u5e93\u540d\u79f0\u4e2d\u5305\u542b <code>-GGUF</code> \u7684\u90e8\u5206\u3002\u8981\u4e0b\u8f7d\u6240\u9700\u7684 GGUF \u6a21\u578b\uff0c\u8bf7\u4f7f\u7528 <code>huggingface-cli</code> \uff08\u9996\u5148\u9700\u8981\u901a\u8fc7\u547d\u4ee4 <code>pip install huggingface_hub</code> \u5b89\u88c5\u5b83\uff09\uff1a</p> <pre><code># huggingface-cli download &lt;model_repo&gt; &lt;gguf_file&gt; \\\n#   --local-dir &lt;local_dir&gt; \\\n#   --local-dir-use-symlinks False\nhuggingface-cli download Qwen/Qwen2-7B-Instruct-GGUF qwen2-7b-instruct-q5_k_m.gguf \\\n    --local-dir . --local-dir-use-symlinks False\n</code></pre> <p>\u7136\u540e\u4f60\u53ef\u4ee5\u7528\u5982\u4e0b\u547d\u4ee4\u8fd0\u884c\u6a21\u578b\uff08\u4f60\u53ef\u4ee5\u4f7f\u7528 <code>./llama-cli -h</code> \u83b7\u53d6\u66f4\u591a\u8d85\u53c2\u6570\u7684\u89e3\u91ca\uff09\uff1a</p> <pre><code>./llama-cli -m qwen2-7b-instruct-q5_k_m.gguf \\\n    -n 512 \\  # (1)!\n    -co -i -if -f prompts/chat-with-qwen.txt \\\n    --in-prefix \"&lt;|im_start|&gt;user\\n\" \\\n    --in-suffix \"&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n\" \\\n    -ngl 80 -fa\n</code></pre> <ol> <li>\u751f\u6210\u7684\u6700\u5927 token \u6570\u91cf</li> </ol>"},{"location":"v2/llama.cpp/#3-gguf","title":"3. \u751f\u6210 GGUF \u6587\u4ef6","text":""},{"location":"v2/llama.cpp/#4-ppl","title":"4. PPL \u8bc4\u6d4b","text":"<p>llama.cpp \u4e3a\u6211\u4eec\u63d0\u4f9b\u4e86\u8bc4\u4f30 GGUF \u6a21\u578b PPL \u6027\u80fd\u7684\u65b9\u6cd5\u3002\u4e3a\u4e86\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u4f60\u9700\u8981\u51c6\u5907\u4e00\u4e2a\u6570\u636e\u96c6\uff0c\u6bd4\u5982 \u201cwiki\u6d4b\u8bd5\u201d\u3002</p> PPL \u6d4b\u8bd5 \u4e0b\u8f7d\u6570\u636e\u96c6\u8fd0\u884c\u6d4b\u8bd5\u8f93\u51fa\u7ed3\u679c <pre><code>wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip?ref=salesforce-research -O wikitext-2-raw-v1.zip\nunzip wikitext-2-raw-v1.zip\n</code></pre> <pre><code>./llama-perplexity -m &lt;gguf_path&gt; -f wiki.test.raw\n</code></pre> <pre><code>perplexity : calculating perplexity over 655 chunks\n24.43 seconds per pass - ETA 4.45 hours\n[1]4.5970,[2]5.1807,[3]6.0382,...\n</code></pre>"},{"location":"v2/llama.cpp/#5-lm-studio-gguf","title":"5. LM Studio \u4e2d\u4f7f\u7528 GGUF","text":"<p>\u5982\u679c\u4f60\u4ecd\u7136\u89c9\u5f97\u4f7f\u7528 llama.cpp \u6709\u56f0\u96be\uff0c\u6211\u5efa\u8bae\u4f60\u5c1d\u8bd5\u4e00\u4e0b LM Studio \u8fd9\u4e2a\u5e73\u53f0\uff0c\u5b83\u5141\u8bb8\u4f60\u641c\u7d22\u548c\u8fd0\u884c\u672c\u5730\u7684\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u3002\u76ee\u524d Qwen2 \u5df2\u7ecf\u6b63\u5f0f\u6210\u4e3a LM Studio\u7684\u4e00\u90e8\u5206\u3002</p>"},{"location":"v2/llamaindex/","title":"LlamaIndex","text":"<p>\u4e3a\u4e86\u5b9e\u73b0 Qwen2 \u4e0e\u5916\u90e8\u6570\u636e\uff08\u4f8b\u5982\u6587\u6863\u3001\u7f51\u9875\u7b49\uff09\u7684\u8fde\u63a5\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86 LlamaIndex \u7684\u8be6\u7ec6\u6559\u7a0b\u3002\u672c\u6307\u5357\u65e8\u5728\u5e2e\u52a9\u7528\u6237\u5229\u7528 LlamaIndex \u4e0e Qwen2 \u5feb\u901f\u90e8\u7f72\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\u3002</p>"},{"location":"v2/llamaindex/#1","title":"1. \u5b89\u88c5","text":"<p>\u4e3a\u5b9e\u73b0\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\uff0c\u6211\u4eec\u5efa\u8bae\u60a8\u9996\u5148\u5b89\u88c5\u4e0e LlamaIndex \u76f8\u5173\u7684\u8f6f\u4ef6\u5305\u3002</p> <pre><code>pip install llama-index\npip install llama-index-llms-huggingface\npip install llama-index-readers-web\n</code></pre>"},{"location":"v2/llamaindex/#2","title":"2. \u8bbe\u7f6e\u53c2\u6570","text":"<p>\u73b0\u5728\uff0c\u6211\u4eec\u53ef\u4ee5\u8bbe\u7f6e\u8bed\u8a00\u6a21\u578b\u548c\u5411\u91cf\u6a21\u578b\u3002Qwen2-Instruct \u652f\u6301\u5305\u62ec\u82f1\u8bed\u548c\u4e2d\u6587\u5728\u5185\u7684\u591a\u79cd\u8bed\u8a00\u5bf9\u8bdd\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528 <code>bge-base-en-v1.5</code> \u6a21\u578b\u6765\u68c0\u7d22\u82f1\u6587\u6587\u6863\uff0c\u4e0b\u8f7d <code>bge-base-zh-v1.5</code> \u6a21\u578b\u4ee5\u68c0\u7d22\u4e2d\u6587\u6587\u6863\u3002\u6839\u636e\u60a8\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u60a8\u8fd8\u53ef\u4ee5\u9009\u62e9<code>bge-large</code> \u6216 <code>bge-small</code> \u4f5c\u4e3a\u5411\u91cf\u6a21\u578b\uff0c\u6216\u8c03\u6574\u4e0a\u4e0b\u6587\u7a97\u53e3\u5927\u5c0f\u6216\u6587\u672c\u5757\u5927\u5c0f\u3002Qwen2\u6a21\u578b\u7cfb\u5217\u652f\u6301\u6700\u5927 32K \u4e0a\u4e0b\u6587\u7a97\u53e3\u5927\u5c0f\uff08Qwen2-7B-Instruct \u4e0eQwen2-72B-Instruct \u53ef\u652f\u6301 128K \u4e0a\u4e0b\u6587\uff0c\u4f46\u9700\u8981\u989d\u5916\u914d\u7f6e\uff09\u3002</p> <pre><code>import torch\nfrom llama_index.core import Settings\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom llama_index.llms.huggingface import HuggingFaceLLM\nfrom llama_index.embeddings.huggingface import HuggingFaceEmbedding\n\n# Set prompt template for generation (optional)\nfrom llama_index.core import PromptTemplate\n\ndef completion_to_prompt(completion):\n   return f\"&lt;|im_start|&gt;system\\n&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\n{completion}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n\"\n\ndef messages_to_prompt(messages):\n    prompt = \"\"\n    for message in messages:\n        if message.role == \"system\":\n            prompt += f\"&lt;|im_start|&gt;system\\n{message.content}&lt;|im_end|&gt;\\n\"\n        elif message.role == \"user\":\n            prompt += f\"&lt;|im_start|&gt;user\\n{message.content}&lt;|im_end|&gt;\\n\"\n        elif message.role == \"assistant\":\n            prompt += f\"&lt;|im_start|&gt;assistant\\n{message.content}&lt;|im_end|&gt;\\n\"\n\n    if not prompt.startswith(\"&lt;|im_start|&gt;system\"):\n        prompt = \"&lt;|im_start|&gt;system\\n\" + prompt\n\n    prompt = prompt + \"&lt;|im_start|&gt;assistant\\n\"\n\n    return prompt\n\n# Set Qwen2 as the language model and set generation config\nSettings.llm = HuggingFaceLLM(\n    model_name=\"Qwen/Qwen2-7B-Instruct\",\n    tokenizer_name=\"Qwen/Qwen2-7B-Instruct\",\n    context_window=30000,\n    max_new_tokens=2000,\n    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n    messages_to_prompt=messages_to_prompt,\n    completion_to_prompt=completion_to_prompt,\n    device_map=\"auto\",\n)\n\n# Set embedding model\nSettings.embed_model = HuggingFaceEmbedding(\n    model_name = \"BAAI/bge-base-en-v1.5\"\n)\n\n# Set the size of the text chunk for retrieval\nSettings.transformations = [SentenceSplitter(chunk_size=1024)]\n</code></pre>"},{"location":"v2/llamaindex/#2_1","title":"2. \u6784\u5efa\u7d22\u5f15","text":"<p>\u4ee5\u4e0b\u4ee3\u7801\u7247\u6bb5\u5c55\u793a\u4e86\u5982\u4f55\u4e3a\u672c\u5730\u540d\u4e3a document \u7684\u6587\u4ef6\u5939\u4e2d\u7684\u6587\u4ef6\uff08\u65e0\u8bba\u662f PDF \u683c\u5f0f\u8fd8\u662fTXT \u683c\u5f0f\uff09\u6784\u5efa\u7d22\u5f15\u3002</p> <pre><code>from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleDirectoryReader(\"./document\").load_data()\nindex = VectorStoreIndex.from_documents(\n    documents,\n    embed_model=Settings.embed_model,\n    transformations=Settings.transformations\n)\n</code></pre> <p>\u4ee5\u4e0b\u4ee3\u7801\u7247\u6bb5\u5c55\u793a\u4e86\u5982\u4f55\u4e3a\u4e00\u7cfb\u5217\u7f51\u7ad9\u7684\u5185\u5bb9\u6784\u5efa\u7d22\u5f15\u3002</p> <pre><code>from llama_index.readers.web import SimpleWebPageReader\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n\ndocuments = SimpleWebPageReader(html_to_text=True).load_data(\n    [\"web_address_1\",\"web_address_2\",...]\n)\nindex = VectorStoreIndex.from_documents(\n    documents,\n    embed_model=Settings.embed_model,\n    transformations=Settings.transformations\n)\n</code></pre> <p>\u8981\u4fdd\u5b58\u548c\u52a0\u8f7d\u5df2\u6784\u5efa\u7684\u7d22\u5f15\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u793a\u4f8b\u3002</p> <pre><code>from llama_index.core import StorageContext, load_index_from_storage\n\n# save index\nstorage_context = StorageContext.from_defaults(persist_dir=\"save\")\n\n# load index\nindex = load_index_from_storage(storage_context)\n</code></pre>"},{"location":"v2/llamaindex/#4","title":"4. \u68c0\u7d22\u589e\u5f3a","text":"<p>\u73b0\u5728\u60a8\u53ef\u4ee5\u8f93\u5165\u67e5\u8be2\uff0cQwen2 \u5c06\u57fa\u4e8e\u7d22\u5f15\u6587\u6863\u7684\u5185\u5bb9\u63d0\u4f9b\u7b54\u6848\u3002</p> <pre><code>query_engine = index.as_query_engine()\nyour_query = \"&lt;your query here&gt;\"\nprint(query_engine.query(your_query).response)\n</code></pre>"},{"location":"v2/mlx-lm/","title":"MLX-LM","text":"<p>MLX-LM \u80fd\u5728 Apple Silicon \u4e0a\u8fd0\u884c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u9002\u7528\u4e8e MacOS\u3002\u76ee\u524d MLX-LM \u5df2\u652f\u6301 Qwen \u6a21\u578b\uff0c\u6b64\u6b21\u6211\u4eec\u63d0\u4f9b\u76f4\u63a5\u53ef\u7528\u7684\u6a21\u578b\u6587\u4ef6\u3002</p>"},{"location":"v2/mlx-lm/#1","title":"1. \u5b89\u88c5","text":"\u5b89\u88c5 mlx-lm PIPCONDA <pre><code>pip install mlx-lm\n</code></pre> <pre><code>conda install -c conda-forge mlx-lm\n</code></pre>"},{"location":"v2/mlx-lm/#2-mlx","title":"2. \u4f7f\u7528 MLX \u6a21\u578b\u6587\u4ef6","text":"<p>\u6211\u4eec\u5df2\u5728 Hugging Face \u63d0\u4f9b\u4e86\u9002\u7528\u4e8e MLX-LM \u7684\u6a21\u578b\u6587\u4ef6\uff0c\u8bf7\u641c\u7d22\u5e26 -MLX \u7684\u5b58\u50a8\u5e93\u3002</p> <pre><code>from mlx_lm import load, generate\n\nmodel, tokenizer = load(\n    'Qwen/Qwen2-7B-Instruct-MLX',\n    tokenizer_config={\"eos_token\": \"&lt;|im_end|&gt;\"}\n    )\n\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\n\nresponse = generate(\n    model, tokenizer, prompt=text, verbose=True, top_p=0.8, \n    temp=0.7, repetition_penalty=1.05, max_tokens=512\n    )\n</code></pre>"},{"location":"v2/mlx-lm/#3-mlx","title":"3. \u5236\u4f5c MLX \u683c\u5f0f\u6a21\u578b","text":"<p>\u5236\u4f5c MLX \u683c\u5f0f\u7684\u6a21\u578b\u975e\u5e38\u7b80\u5355\uff0c\u4ec5\u4ec5\u9700\u8981\u4e00\u6761\u547d\u4ee4\u5c31\u53ef\u4ee5\u5b8c\u6210\uff1a</p> <pre><code>mlx_lm.convert --hf-path Qwen/Qwen2-7B-Intruct \\  # (1)!\n    --mlx-path mlx/Qwen2-7B-Instruct \\  # (2)!\n    - q  # (3)!\n</code></pre> <ol> <li>HuggingFace Hub \u4e0a\u7684\u6a21\u578b\u540d\u79f0\u6216\u672c\u5730\u8def\u5f84</li> <li>\u8f93\u51fa\u6a21\u578b\u6587\u4ef6\u7684\u5b58\u50a8\u8def\u5f84</li> <li>\u542f\u7528\u91cf\u5316</li> </ol>"},{"location":"v2/ollama/","title":"Ollama","text":"<p>Ollama \u5e2e\u52a9\u60a8\u901a\u8fc7\u5c11\u91cf\u547d\u4ee4\u5373\u53ef\u5728\u672c\u5730\u8fd0\u884c LLM\uff0c\u5b83\u9002\u7528\u4e8e MacOS\u3001Linux \u548c Windows \u64cd\u4f5c\u7cfb\u7edf\u3002\u73b0\u5728\uff0cQwen2 \u6b63\u5f0f\u4e0a\u7ebf Ollama\uff0c\u60a8\u53ea\u9700\u4e00\u6761\u547d\u4ee4\u5373\u53ef\u8fd0\u884c\u5b83\uff1a</p> <pre><code>ollama run qwen2\n</code></pre>"},{"location":"v2/ollama/#1","title":"1. \u5feb\u901f\u5f00\u59cb","text":"<p>\u8bbf\u95ee\u5b98\u65b9\u7f51\u7ad9 Ollama \uff0c\u4e0b\u8f7d\u5e76\u5b89\u88c5 Ollama\u3002\u60a8\u8fd8\u53ef\u4ee5\u5728\u7f51\u7ad9\u4e0a\u641c\u7d22\u6a21\u578b\uff0c\u5728\u8fd9\u91cc\u60a8\u53ef\u4ee5\u627e\u5230 Qwen2 \u7cfb\u5217\u6a21\u578b\u3002</p> <p>\u9664\u4e86\u9ed8\u8ba4\u6a21\u578b\u4e4b\u5916\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u9009\u62e9\u8fd0\u884c\u4e0d\u540c\u5927\u5c0f\uff080.5B\u30011.5B\u30017B \u4ee5\u53ca 72B\uff09\u7684 Qwen2-Instruct \u6a21\u578b\uff1a</p> <pre><code>ollama run qwen2:0.5b\n</code></pre> <p>\u6210\u529f\u542f\u52a8\u670d\u52a1\u540e\uff08\u7aef\u53e3\u542f\u52a8\u5728 11434\uff09\uff0c\u53ef\u4ee5\u901a\u8fc7\u53d1\u9001\u8bf7\u6c42\u6765\u548c Qwen2 \u8fdb\u884c\u4ea4\u4e92\u3002</p> <pre><code>import requests\nimport json\n\nheaders = {\"Content-Type\": \"application/json\"}\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"\u4f60\u597d\"}\n]\nmodel = \"qwen2:0.5b\"\nollama_url = \"http://localhost:11434/v1/chat/completions\"\ndata = {\n    \"model\": model,\n    \"messages\": messages\n}\n\nresp = requests.post(ollama_url, headers=headers, data=json.dumps(data))\nresp\n</code></pre>"},{"location":"v2/ollama/#2-gguf","title":"2. \u8fd0\u884c GGUF \u6587\u4ef6","text":"<p>\u6709\u65f6\u60a8\u53ef\u80fd\u4e0d\u60f3\u62c9\u53d6\u6a21\u578b\uff0c\u800c\u662f\u5e0c\u671b\u76f4\u63a5\u4f7f\u7528\u81ea\u5df1\u7684 GGUF \u6587\u4ef6\u6765\u914d\u5408 Ollama\u3002\u5047\u8bbe\u60a8\u6709\u4e00\u4e2a\u540d\u4e3a  qwen2-7b-instruct-q5_0.gguf \u7684 Qwen2 \u7684 GGUF \u6587\u4ef6\u3002\u5728\u7b2c\u4e00\u6b65\u4e2d\uff0c\u60a8\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a Modelfile \u7684\u6587\u4ef6\u3002</p> <pre><code>FROM qwen2-7b-instruct-q5_0.gguf\n\n# set the temperature to 1 [higher is more creative, lower is more coherent]\nPARAMETER temperature 0.7\nPARAMETER top_p 0.8\nPARAMETER repeat_penalty 1.05\nPARAMETER top_k 20\n\nTEMPLATE \"\"\"{{ if and .First .System }}&lt;|im_start|&gt;system\n{{ .System }}&lt;|im_end|&gt;\n{{ end }}&lt;|im_start|&gt;user\n{{ .Prompt }}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n{{ .Response }}\"\"\"\n\n# set the system message\nSYSTEM \"\"\"\nYou are a helpful assistant.\n\"\"\"\n</code></pre> <p>\u7136\u540e\u901a\u8fc7\u8fd0\u884c\u4e0b\u5217\u547d\u4ee4\u6765\u521b\u5efa\u4e00\u4e2a Ollama \u6a21\u578b</p> <pre><code>ollama create qwen2_7b -f Modelfile\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u8fd0\u884c\u81ea\u5df1\u7684 Ollama \u6a21\u578b\u4e86\uff1a</p> <pre><code>ollama run qwen2_7b\n</code></pre>"},{"location":"v2/quantization_benchmark/","title":"\u6548\u679c\u8bc4\u4f30","text":"<p>\u672c\u90e8\u5206\u4ecb\u7ecd Qwen2 \u91cf\u5316\u6a21\u578b\uff08\u5305\u62ec GPTQ \u4e0e AWQ \u91cf\u5316\u65b9\u6848\uff09\u7684\u6548\u679c\u8bc4\u4f30\uff0c\u6709\u4ee5\u4e0b\u6570\u636e\u96c6</p> <ul> <li>MMLU \uff08\u51c6\u786e\u7387\uff09</li> <li>C-Eval \uff08\u51c6\u786e\u7387\uff09</li> <li>IFEval \uff08\u63d0\u793a\u8bcd\u7ea7\u7684\u4e25\u683c\u51c6\u786e\u7387\uff0cStrict Prompt-Level Accuracy\uff09</li> </ul> <p>\u6240\u6709\u6a21\u578b\u5747\u4f7f\u7528\u8d2a\u5fc3\u89e3\u7801\u3002</p> Quantization Average MMLU C-Eval IFEval Qwen2-72B-Instruct BF16 81.3 82.3 83.8 77.6 GPTQ-Int8 80.7 81.3 83.4 77.5 GPTQ-Int4 81.2 80.8 83.9 78.9 AWQ 80.4 80.5 83.9 76.9 Qwen2-7B-Instruct BF16 66.9 70.5 77.2 53.1 GPTQ-Int8 66.2 69.1 76.7 52.9 GPTQ-Int4 64.1 67.8 75.2 49.4 AWQ 64.1 67.4 73.6 51.4 Qwen2-1.5B-Instruct BF16 48.4 52.4 63.8 29.0 GPTQ-Int8 48.1 53.0 62.5 28.8 GPTQ-Int4 45.0 50.7 57.4 27.0 AWQ 46.5 51.6 58.1 29.9 Qwen2-0.5B-Instruct BF16 34.4 37.9 45.2 20.0 GPTQ-Int8 32.6 35.6 43.9 18.1 GPTQ-Int4 29.7 33.0 39.2 16.8 AWQ 31.1 34.4 42.1 16.7"},{"location":"v2/quickstart/","title":"\u5feb\u901f\u5165\u95e8","text":"<p>\u5b89\u88c5\u5b8c <code>transformers</code> \u5e93\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u4e00\u4e9b\u793a\u4f8b\uff08\u5305\u542b HuggingFace \u4ee5\u53ca vLLM \u90e8\u7f72\uff09\u6765\u5feb\u901f\u4e86\u89e3\u4e0b Qwen2 \u7684\u4f7f\u7528\u3002</p>"},{"location":"v2/quickstart/#1-huggingface-modelscope-transformers","title":"1. HuggingFace &amp; ModelScope Transformers","text":"<p>\u8981\u5feb\u901f\u4e0a\u624b Qwen2\uff0c\u6211\u4eec\u5efa\u8bae\u60a8\u9996\u5148\u5c1d\u8bd5\u4f7f\u7528 <code>transformers</code> \u8fdb\u884c\u63a8\u7406\uff0c\u8bf7\u786e\u4fdd\u5df2\u5b89\u88c5\u4e86 <code>transformers&gt;=4.40.0</code> \u7248\u672c\u3002\u4ee5\u4e0b\u662f\u4e00\u4e2a\u975e\u5e38\u7b80\u5355\u7684\u4ee3\u7801\u7247\u6bb5\u793a\u4f8b\uff0c\u5c55\u793a\u5982\u4f55\u8fd0\u884cQwen2-Instruct \u6a21\u578b\uff0c\u5176\u4e2d\u5305\u542b Qwen2-7B-Instruct \u7684\u5b9e\u4f8b\uff1a</p> <pre><code>from transformers import AutoModelForCausalLM, AutoTokenizer  # (1)!\n\n\ndevice = \"cuda\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2-7B-Instruct\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\"\n)\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")\n\n# \u4e0d\u540c\u4e8e\u4ee5\u524d\u4f7f\u7528 \u7684`model.chat` \u65b9\u6cd5\uff0c\u73b0\u5728\u6211\u4eec\u4f7f\u7528 `model.generate` \u65b9\u6cd5\n# \u4f46\u662f\u6211\u4eec\u9700\u8981\u5148\u4f7f\u7528 `tokenizer.apply_chat_template` \u65b9\u6cd5\u6765\u683c\u5f0f\u5316\u8f93\u5165\nprompt = \"Give me a short introduction to large language model.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(  # (2)!\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n\n# \u76f4\u63a5\u4f7f\u7528 `model.generate` \u4ee5\u53ca `tokenizer.decode` \u65b9\u6cd5\u83b7\u53d6\u8f93\u51fa\ngenerated_ids = model.generate(  # (3)!\n    model_inputs.input_ids,\n    max_new_tokens=512  # (4)!\n)\ngenerated_ids = [\n    output_ids[len(input_ids):]   # (5)!\n    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n]\n\nresponse = tokenizer.batch_decode(  # (6)!\n    generated_ids, skip_special_tokens=True\n  )[0]\n</code></pre> <ol> <li> <p>\u7531\u4e8e HuggingFace \u5728\u56fd\u5185\u4e0b\u8f7d\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u6211\u4eec\u63a8\u8350\u4ece ModelScope \u4e0a\u8fdb\u884c\u4e0b\u8f7d\uff0c\u53ea\u9700\u8981\u66f4\u6539\u4e3a\u4ee5\u4e0b\u4ee3\u7801\u5373\u53ef\u3002</p> <pre><code>from modelscope import AutoModelForCausalLM, AutoTokenizer\n</code></pre> </li> <li> <p>\u5148\u901a\u8fc7 <code>tokenizer.apply_chat_template</code> \u65b9\u6cd5\u5bf9\u8f93\u5165\u8fdb\u884c\u683c\u5f0f\u5316\uff0c\u8fd9\u4e5f\u662f <code>transformers</code> \u63a8\u8350\u7684\u5b9e\u8df5\u65b9\u5f0f\u3002</p> </li> <li>\u76f4\u63a5\u8c03\u7528 <code>model.generate</code> \u65b9\u6cd5\u751f\u6210\u8f93\u51fa\u3002\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u8fd9\u91cc\u8f93\u51fa\u7684\u5e76\u4e0d\u662f\u6700\u7ec8\u7684\u6587\u672c\u5185\u5bb9\uff0c\u800c\u662f\u5305\u542b\u4e86\u8f93\u5165\u7684\u7f16\u7801\u4fe1\u606f\u3002</li> <li>\u4f7f\u7528 <code>max_new_tokens</code> \u53c2\u6570\u9650\u5236\u8f93\u51fa\u50ac\u6700\u5927\u957f\u5ea6\u3002</li> <li>\u901a\u5e38\u6211\u4eec\u53ea\u5173\u5fc3\u8f93\u51fa\u7684\u5185\u5bb9\uff0c\u6240\u4ee5\u8fd9\u91cc\u4ece\u8f93\u5165\u957f\u5ea6\u5f00\u59cb\u622a\u53d6\u3002</li> <li>\u5bf9\u622a\u65ad\u540e\u7684\u7f16\u7801\u8fdb\u884c\u89e3\u7801\uff0c\u5f97\u5230\u5b9e\u9645\u7684\u8f93\u51fa\u5185\u5bb9\u3002</li> </ol> <p>\u5982\u679c\u4f60\u60f3\u4f7f\u7528 Flash Attention 2 (1)\uff0c\u4f60\u53ef\u4ee5\u7528\u4e0b\u9762\u8fd9\u79cd\u65b9\u5f0f\u8bfb\u53d6\u6a21\u578b\uff1a</p> <ol> <li>\u5f3a\u70c8\u5efa\u8bae\u5b89\u88c5 Flash Attention 2\uff0c\u4e0d\u7136 Qwen \u7684\u63a8\u7406\u901f\u5ea6\u5b9e\u9645\u4e0a\u4e5f\u662f\u6bd4\u8f83\u6162\u7684\u3002</li> </ol> <pre><code>model = AutoModelForCausalLM.from_pretrained(\n    \"Qwen/Qwen2-7B-Instruct\",\n    torch_dtype=\"auto\",\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",\n)\n</code></pre> <p>\u6b64\u5916\uff0c\u7ed3\u5408 <code>transformers</code> \u7684 <code>TextStreamer</code> \u7c7b\uff0c\u53ef\u4ee5\u8f7b\u677e\u5b9e\u73b0\u6d41\u5f0f\u8f93\u51fa\uff1a</p> <pre><code>from transformers import TextStreamer\nstreamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\ngenerated_ids = model.generate(\n    model_inputs.input_ids,\n    max_new_tokens=512,\n    streamer=streamer,\n)\n</code></pre>"},{"location":"v2/quickstart/#2-vllm","title":"2. \u4f7f\u7528 vLLM \u90e8\u7f72","text":"<p>\u8981\u90e8\u7f72 Qwen2\uff0c\u6211\u4eec\u5efa\u8bae\u60a8\u4f7f\u7528 vLLM\u3002vLLM \u662f\u4e00\u4e2a\u7528\u4e8e LLM \u63a8\u7406\u548c\u670d\u52a1\u7684\u5feb\u901f\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u6846\u67b6\u3002\u4ee5\u4e0b\uff0c\u6211\u4eec\u5c06\u5c55\u793a\u5982\u4f55\u4f7f\u7528 vLLM \u6784\u5efa\u4e00\u4e2a\u4e0e OpenAI API \u517c\u5bb9\u7684 API \u670d\u52a1\u3002</p> vLLM \u7684\u5b89\u88c5\u4e0e\u8fd0\u884c \u5b89\u88c5 vLLM\u8fd0\u884c vLLM \u670d\u52a1 <p>\u9700\u8981\u786e\u4fdd\u5b89\u88c5\u7684 vLLM \u7248\u672c\u5728 0.4.0 \u4ee5\u4e0a\u3002</p> <pre><code>pip install vllm\n</code></pre> <pre><code>python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2-7B-Instruct\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u53ef\u4ee5 <code>curl</code> \u6216\u8005 OpenAI \u7684 Python \u63a5\u53e3\u6765\u4e0e Qwen \u8fdb\u884c\u4ea4\u4e92\uff1a</p> vLLM \u7684\u5b89\u88c5\u4e0e\u8fd0\u884c \u76f4\u63a5\u4f7f\u7528 curl\u8c03\u7528 OpenAI \u7684\u63a5\u53e3 <pre><code>curl http://localhost:8000/v1/chat/completions \\ \n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"Qwen/Qwen2-7B-Instruct\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"}\n    ]\n  }'\n</code></pre> <pre><code>from openai import OpenAI\n# \u8bbe\u7f6e OpenAI \u7684 Key \u548c API \u63a5\u53e3\uff08\u672c\u5730\u5c31\u662f vLLM \u542f\u52a8\u7aef\u53e3\uff09\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nchat_response = client.chat.completions.create(\n    model=\"Qwen/Qwen2-7B-Instruct\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"},\n      ]\n    )\nprint(\"Chat response:\", chat_response)\n</code></pre>"},{"location":"v2/qwen_agent/","title":"Qwen Agent","text":"<p>Qwen-Agent \u662f\u4e00\u4e2a\u57fa\u4e8e Qwen \u7684\u6307\u4ee4\u8ddf\u968f\u3001\u5de5\u5177\u4f7f\u7528\u3001\u8ba1\u5212\u548c\u8bb0\u5fc6\u80fd\u529b\u6765\u5f00\u53d1 LLM \u5e94\u7528\u7a0b\u5e8f\u7684\u6846\u67b6\u3002\u5b83\u8fd8\u9644\u5e26\u4e86\u4e00\u4e9b\u793a\u4f8b\u5e94\u7528\u7a0b\u5e8f\uff0c\u4f8b\u5982\u6d4f\u89c8\u5668\u52a9\u624b\u3001\u4ee3\u7801\u89e3\u91ca\u5668\u548c\u81ea\u5b9a\u4e49\u52a9\u624b\u3002</p>"},{"location":"v2/qwen_agent/#1","title":"1. \u5b89\u88c5","text":"<pre><code>git clone https://github.com/QwenLM/Qwen-Agent.git\ncd Qwen-Agent\npip install -e ./\n</code></pre>"},{"location":"v2/qwen_agent/#2","title":"2. \u5f00\u53d1\u81ea\u5b9a\u4e49\u667a\u80fd\u4f53","text":"<p>Qwen-Agent \u63d0\u4f9b\u5305\u62ec\u8bed\u8a00\u6a21\u578b\u548c\u63d0\u793a\u8bcd\u7b49\u539f\u5b50\u7ea7\u7ec4\u4ef6\uff0c\u53ca\u667a\u80fd\u4f53\u7b49\u9ad8\u7ea7\u7ec4\u4ef6\u5728\u5185\u7684\u591a\u79cd\u7ec4\u4ef6\u3002\u4ee5\u4e0b\u793a\u4f8b\u9009\u53d6\u52a9\u7406\u7ec4\u4ef6\u8fdb\u884c\u5c55\u793a\uff0c\u9610\u8ff0\u4e86\u5982\u4f55\u6574\u5408\u81ea\u5b9a\u4e49\u5de5\u5177\u4ee5\u53ca\u5982\u4f55\u8fc5\u901f\u5f00\u53d1\u51fa\u4e00\u4e2a\u80fd\u591f\u5e94\u7528\u8fd9\u4e9b\u5de5\u5177\u7684\u4ee3\u7406\u7a0b\u5e8f\u3002</p> <pre><code>import json\nimport os\n\nimport json5\nimport urllib.parse\nfrom qwen_agent.agents import Assistant\nfrom qwen_agent.tools.base import BaseTool, register_tool\n\nllm_cfg = {\n    # Use the model service provided by DashScope:\n    'model': 'qwen-max',\n    'model_server': 'dashscope',\n    # 'api_key': 'YOUR_DASHSCOPE_API_KEY',\n    # It will use the `DASHSCOPE_API_KEY' environment variable if 'api_key' is not set here.\n\n    # Use your own model service compatible with OpenAI API:\n    # 'model': 'Qwen/Qwen2-72B-Instruct',\n    # 'model_server': 'http://localhost:8000/v1',  # api_base\n    # 'api_key': 'EMPTY',\n\n    # (Optional) LLM hyperparameters for generation:\n    'generate_cfg': {\n        'top_p': 0.8\n    }\n}\nsystem = 'According to the user\\'s request, you first draw a picture and then automatically run code to download the picture ' + \\\n          'and select an image operation from the given document to process the image'\n\n# Add a custom tool named my_image_gen\uff1a\n@register_tool('my_image_gen')\nclass MyImageGen(BaseTool):\n    description = 'AI painting (image generation) service, input text description, and return the image URL drawn based on text information.'\n    parameters = [{\n        'name': 'prompt',\n        'type': 'string',\n        'description': 'Detailed description of the desired image content, in English',\n        'required': True\n    }]\n\n    def call(self, params: str, **kwargs) -&gt; str:\n        prompt = json5.loads(params)['prompt']\n        prompt = urllib.parse.quote(prompt)\n        return json.dumps(\n            {'image_url': f'https://image.pollinations.ai/prompt/{prompt}'},\n            ensure_ascii=False)\n\n\ntools = ['my_image_gen', 'code_interpreter']  # code_interpreter is a built-in tool in Qwen-Agent\nbot = Assistant(llm=llm_cfg,\n                system_message=system,\n                function_list=tools,\n                files=[os.path.abspath('doc.pdf')])\n\nmessages = []\nwhile True:\n    query = input('user question: ')\n    messages.append({'role': 'user', 'content': query})\n    response = []\n    for response in bot.run(messages=messages):\n        print('bot response:', response)\n    messages.extend(response)\n</code></pre> <p>\u6211\u4eec\u5728\u5b98\u65b9\u4ed3\u5e93\u4e2d\u4e86\u89e3\u66f4\u591a\u4f7f\u7528\u6848\u4f8b\u3002</p>"},{"location":"v2/sft_example/","title":"\u5fae\u8c03\u793a\u4f8b","text":"<p>Qwen \u63d0\u4f9b\u4e86\u4e00\u4e2a\u975e\u5e38\u7b80\u5355\u7684\u6709\u76d1\u7763\u5fae\u8c03\u811a\u672c\uff08\u8be5\u811a\u672c\u662f\u57fa\u4e8e Fastchat \u7684\u8bad\u7ec3\u811a\u672c\u4fee\u6539\u800c\u6765\uff09\uff0c\u5b83\u4f7f\u7528 Hugging Face Trainer \u5bf9 Qwen \u6a21\u578b\u8fdb\u884c\u5fae\u8c03\u3002\u4f60\u53ef\u4ee5\u67e5\u770b\u8fd9\u4e2a\u811a\u672c \u5b9e\u73b0\u7ec6\u8282\uff0c\u6b64\u5916\u8fd9\u4e2a\u811a\u672c\u8fd8\u5177\u6709\u4ee5\u4e0b\u7279\u70b9\uff1a</p> <ul> <li>\u652f\u6301\u5355\u5361\u548c\u591a\u5361\u5206\u5e03\u5f0f\u8bad\u7ec3</li> <li>\u652f\u6301\u5168\u53c2\u6570\u5fae\u8c03\u3001LoRA \u4ee5\u53ca Q-LoRA\u3002</li> </ul> <p>\u4e0b\u9762\uff0c\u6211\u4eec\u4ecb\u7ecd\u811a\u672c\u7684\u66f4\u591a\u7ec6\u8282\u3002</p>"},{"location":"v2/sft_example/#1","title":"1. \u5b89\u88c5","text":"<p>\u5f00\u59cb\u4e4b\u524d\uff0c\u786e\u4fdd\u4f60\u5df2\u7ecf\u5b89\u88c5\u4e86\u4ee5\u4e0b\u4ee3\u7801\u5e93\uff1a</p> <pre><code>pip install peft deepspeed optimum accelerate\n</code></pre>"},{"location":"v2/sft_example/#2","title":"2. \u51c6\u5907\u6570\u636e","text":"<p>\u6211\u4eec\u5efa\u8bae\u4f60\u653e\u5728 jsonl \u6587\u4ef6\u4e2d\uff0c\u6bcf\u4e00\u884c\u662f\u4e00\u4e2a\u5982\u4e0b\u6240\u793a\u7684\u5b57\u5178\uff1a</p> <pre><code>[\n  {\n    \"type\": \"chatml\",\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"Tell me something about large language models.\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"Large language models are a type of language model that is trained on a large corpus of text data. They are capable of generating human-like text and are used in a variety of natural language processing tasks...\"\n        }\n    ],\n    \"source\": \"unknown\"\n  },\n  {\n    \"type\": \"chatml\",\n    \"messages\": [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant.\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": \"What is your name?\"\n        },\n        {\n            \"role\": \"assistant\",\n            \"content\": \"My name is Qwen.\"\n        }\n    ],\n    \"source\": \"self-made\"\n  }\n]\n</code></pre> <p>\u4ee5\u4e0a\u63d0\u4f9b\u4e86\u8be5\u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u6837\u672c\u7684\u4e24\u4e2a\u793a\u4f8b\uff0c\u6bcf\u4e2a\u6837\u672c\u90fd\u662f\u4e00\u4e2a JSON \u5bf9\u8c61\uff0c\u5305\u542b\u4ee5\u4e0b\u5b57\u6bb5\uff1a<code>type</code>\u3001 <code>messages</code> \u548c <code>source</code>\u3002\u5176\u4e2d\uff0c<code>messages</code> \u662f\u5fc5\u586b\u5b57\u6bb5\uff0c\u800c\u5176\u4ed6\u5b57\u6bb5\u5219\u662f\u4f9b\u60a8\u6807\u8bb0\u6570\u636e\u683c\u5f0f\u548c\u6570\u636e\u6765\u6e90\u7684\u53ef\u9009\u5b57\u6bb5\u3002<code>messages</code> \u5b57\u6bb5\u662f\u4e00\u4e2a JSON \u5bf9\u8c61\u5217\u8868\uff0c\u6bcf\u4e2a\u5bf9\u8c61\u90fd\u5305\u542b\u4e24\u4e2a\u5b57\u6bb5\uff1a<code>role</code> \u548c <code>content</code>\u3002\u5176\u4e2d\uff0c<code>role</code> \u53ef\u4ee5\u662f <code>system</code>\u3001<code>user</code> \u6216 <code>assistant</code>\uff0c\u8868\u793a\u6d88\u606f\u7684\u89d2\u8272\uff1b<code>content</code> \u5219\u662f\u6d88\u606f\u7684\u6587\u672c\u5185\u5bb9\u3002\u800c <code>source</code> \u5b57\u6bb5\u4ee3\u8868\u4e86\u6570\u636e\u6765\u6e90\uff0c\u53ef\u80fd\u5305\u62ec <code>self-made</code>\u3001<code>alpaca</code>\u3001<code>open-hermes</code> \u6216\u5176\u4ed6\u4efb\u610f\u5b57\u7b26\u4e32\u3002</p> <p>\u4f60\u9700\u8981\u7528 json \u5c06\u4e00\u4e2a\u5b57\u5178\u5217\u8868\u5b58\u5165 jsonl \u6587\u4ef6\u4e2d\uff1a</p> <pre><code>import json\n\nwith open('data.jsonl', 'w') as f:\n    for sample in samples:\n        f.write(json.dumps(sample) + '\\n')\n</code></pre>"},{"location":"v2/sft_example/#3","title":"3. \u5f00\u59cb\u5fae\u8c03","text":"<p>\u4e3a\u4e86\u8ba9\u60a8\u80fd\u591f\u5feb\u901f\u5f00\u59cb\u5fae\u8c03\uff0c\u6211\u4eec\u76f4\u63a5\u63d0\u4f9b\u4e86\u4e00\u4e2a Shell \u811a\u672c\uff0c\u60a8\u53ef\u4ee5\u65e0\u9700\u5173\u6ce8\u5177\u4f53\u7ec6\u8282\u5373\u53ef\u8fd0\u884c\u3002\u9488\u5bf9\u4e0d\u540c\u7c7b\u578b\u7684\u8bad\u7ec3\uff08\u4f8b\u5982\u5355 GPU \u8bad\u7ec3\u3001\u591a GPU \u8bad\u7ec3\u3001\u5168\u53c2\u6570\u5fae\u8c03\u3001LoRA \u6216 Q-LoRA\uff09\uff0c\u60a8\u53ef\u80fd\u9700\u8981\u4e0d\u540c\u7684\u8d85\u53c2\u6570\u8bbe\u7f6e\u3002</p> <pre><code>cd examples/sft\nbash finetune.sh -m &lt;model_path&gt; -d &lt;data_path&gt; \\\n  --deepspeed &lt;config_path&gt; \\ # (1)!\n  [--use_lora True] \\\n  [--q_lora True]\n</code></pre> <ol> <li>\u6307\u5b9a Deepspeed \u914d\u7f6e\u6587\u4ef6</li> </ol>"},{"location":"v2/sft_example/#4","title":"4. \u9ad8\u9636\u7528\u6cd5","text":"<p>\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u4ecb\u7ecd\u4e0b Python \u4ee5\u53ca Shell \u811a\u672c\u7684\u4ee3\u7801\u7ec6\u8282\u3002</p>"},{"location":"v2/sft_example/#41-shell","title":"4.1 Shell \u811a\u672c","text":"<p>Shell \u811a\u672c\u4e2d\u63d0\u4f9b\u4e86\u4e00\u4e9b\u6307\u5357\uff0c\u5e76\u4e14\u6b64\u5904\u5c06\u4ee5 finetune.sh \u8fd9\u4e2a\u811a\u672c\u4e3a\u4f8b\u8fdb\u884c\u89e3\u91ca\u8bf4\u660e\u3002</p> <p>\u8981\u4e3a\u5206\u5e03\u5f0f\u8bad\u7ec3\uff08\u6216\u5355 GPU \u8bad\u7ec3\uff09\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf\uff0c\u8bf7\u6307\u5b9a\u4ee5\u4e0b\u53d8\u91cf\uff1a<code>GPUS_PER_NODE</code>\u3001 <code>NNODES</code>\u3001<code>NODE_RANK</code>\u3001<code>MASTER_ADDR</code> \u548c <code>MASTER_PORT</code>\u3002\u4e0d\u5fc5\u8fc7\u4e8e\u62c5\u5fc3\u8fd9\u4e9b\u53d8\u91cf\uff0c\u56e0\u4e3a\u6211\u4eec\u4e3a\u60a8\u63d0\u4f9b\u4e86\u9ed8\u8ba4\u8bbe\u7f6e\u3002\u5728\u547d\u4ee4\u884c\u4e2d\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7\u4f20\u5165\u53c2\u6570 <code>-m</code> \u548c <code>-d</code> \u6765\u5206\u522b\u6307\u5b9a\u6a21\u578b\u8def\u5f84\u548c\u6570\u636e\u8def\u5f84\u3002\u60a8\u8fd8\u53ef\u4ee5\u901a\u8fc7\u4f20\u5165\u53c2\u6570 <code>--deepspeed</code> \u6765\u6307\u5b9a Deepspeed \u914d\u7f6e\u6587\u4ef6\u3002\u6211\u4eec\u4e3a\u60a8\u63d0\u4f9b\u9488\u5bf9 ZeRO2 \u548c ZeRO3 \u7684\u4e24\u79cd\u914d\u7f6e\u6587\u4ef6\uff0c\u60a8\u53ef\u4ee5\u6839\u636e\u9700\u6c42\u9009\u62e9\u5176\u4e2d\u4e4b\u4e00\u3002\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u5efa\u8bae\u5728\u591a GPU \u8bad\u7ec3\u4e2d\u4f7f\u7528 ZeRO3\uff0c\u4f46\u9488\u5bf9Q-LoRA\uff0c\u6211\u4eec\u63a8\u8350\u4f7f\u7528 ZeRO2\u3002</p> <p>\u6211\u4eec\u53ef\u4ee5\u6307\u5b9a <code>--bf16</code> \u6216 <code>--fp16</code> \u53c2\u6570\u6765\u8bbe\u7f6e\u6df7\u5408\u7cbe\u5ea6\u8bad\u7ec3\u6240\u91c7\u7528\u7684\u7cbe\u5ea6\u7ea7\u522b\u3002\u6b64\u5916\uff0c\u8fd8\u6709\u5176\u4ed6\u4e00\u4e9b\u91cd\u8981\u7684\u8d85\u53c2\u6570\u5982\u4e0b\uff1a</p> <ul> <li><code>--output_dir</code>: \u6a21\u578b\u6216\u8005 Adapters \u4fdd\u5b58\u8def\u5f84</li> <li><code>--num_train_epochs</code>: \u8bad\u7ec3\u5468\u671f\u6570</li> <li><code>--gradient_accumulation_steps</code>: \u591a\u5c11\u6b65\u8fdb\u884c\u4e00\u6b21\u68af\u5ea6\u7d2f\u52a0</li> <li><code>--per_device_train_batch_size</code>: \u6307\u5b9a\u6bcf\u4e2a GPU \u7684\u8bad\u7ec3\u6279\u6b21\u5927\u5c0f\uff0c\u603b\u7684\u6279\u6b21\u5927\u5c0f\u7b49\u4e8e <code>per_device_train_batch_size</code> \u00d7 <code>number_of_gpus</code> \u00d7 <code>gradient_accumulation_steps</code>\u3002</li> <li><code>--learning_rate</code>: \u5b66\u4e60\u7387</li> <li><code>--warmup_steps</code>: \u70ed\u542f\u52a8\u7684\u6b65\u6570</li> <li><code>--lr_scheduler_type</code>: \u5b66\u4e60\u7387\u8c03\u5ea6\u5668\u7684\u7c7b\u578b</li> <li><code>--weight_decay</code>: \u6743\u91cd\u8870\u51cf\u7684\u503c</li> <li><code>--addm_beta2</code>: Adam \u4e2d \\(\\beta_2\\) \u7684\u503c</li> <li><code>--model_max_length</code>: \u6700\u5927\u7684\u5e8f\u5217\u957f\u5ea6</li> <li><code>--use_lora</code>: \u662f\u5426\u4f7f\u7528 LoRA\uff0c\u901a\u8fc7\u6307\u5b9a <code>--q_lora</code> \u53c2\u6570\u5f00\u542f Q-LoRA</li> <li><code>--gradient_checkpointing</code>: \u662f\u5426\u4f7f\u7528\u68af\u5ea6\u68c0\u67e5\u70b9</li> </ul>"},{"location":"v2/sft_example/#42-python","title":"4.2 Python \u811a\u672c","text":"<p>\u5728\u672c\u811a\u672c\u4e2d\uff0c\u6211\u4eec\u4e3b\u8981\u4f7f\u7528\u6765\u81ea HF \u7684 <code>trainer</code> \u548c <code>peft</code> \u6765\u8bad\u7ec3\u6211\u4eec\u7684\u6a21\u578b\u3002\u540c\u65f6\uff0c\u6211\u4eec\u4e5f\u5229\u7528 deepspeed \u6765\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\u3002</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass ModelArguments:\n    model_name_or_path: Optional[str] = field(default=\"Qwen/Qwen2-7B\")\n\n\n@dataclass\nclass DataArguments:\n    data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the training data.\"}\n    )\n    eval_data_path: str = field(\n        default=None, metadata={\"help\": \"Path to the evaluation data.\"}\n    )\n    lazy_preprocess: bool = False\n\n\n@dataclass\nclass TrainingArguments(transformers.TrainingArguments):\n    cache_dir: Optional[str] = field(default=None)\n    optim: str = field(default=\"adamw_torch\")\n    model_max_length: int = field(\n        default=8192,\n        metadata={\n            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n        },\n    )\n    use_lora: bool = False\n\n\n@dataclass\nclass LoraArguments:\n    lora_r: int = 64\n    lora_alpha: int = 16\n    lora_dropout: float = 0.05\n    lora_target_modules: List[str] = field(\n        default_factory=lambda: [\n            \"q_proj\",\n            \"k_proj\",\n            \"v_proj\",\n            \"o_proj\",\n            \"up_proj\",\n            \"gate_proj\",\n            \"down_proj\",\n        ]\n    )\n    lora_weight_path: str = \"\"\n    lora_bias: str = \"none\"\n    q_lora: bool = False\n</code></pre> <p>\u6211\u4eec\u901a\u8fc7\u53c2\u6570\u7c7b\u4e3a\u6a21\u578b\u3001\u6570\u636e\u548c\u8bad\u7ec3\u6307\u5b9a\u8d85\u53c2\u6570\uff0c\u5982\u679c\u4f7f\u7528 LoRA \u6216 Q-LoRA \u8bad\u7ec3\u6a21\u578b\uff0c\u8fd8\u4f1a\u5305\u542b\u8fd9\u4e24\u4e2a\u65b9\u6cd5\u7684\u76f8\u5173\u8d85\u53c2\u6570\u3002\u5177\u4f53\u6765\u8bf4\uff0c<code>model-max-length</code> \u662f\u4e00\u4e2a\u5173\u952e\u7684\u8d85\u53c2\u6570\uff0c\u5b83\u51b3\u5b9a\u4e86\u8bad\u7ec3\u6570\u636e\u7684\u6700\u5927\u5e8f\u5217\u957f\u5ea6\u3002</p> <p><code>LoRAArguments</code> \u5305\u542b\u4e86 LoRA \u6216\u8005 Q-LoRA \u7684\u8d85\u53c2\u6570:</p> <ul> <li><code>lora_r</code>: LoRA \u7684\u79e9</li> <li><code>lora_alpha</code>: LoRA \u7684 \\(\\alpha\\) \u503c</li> <li><code>lora_dropout</code>: LoRA \u7684 <code>dropout</code> \u6bd4\u4f8b</li> <li><code>lora_target_modules</code>: LoRA \u7684\u76ee\u6807\u6a21\u5757\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4f1a\u5fae\u8c03\u6240\u6709\u7ebf\u6027\u5c42</li> <li><code>lora_weight_path</code>: LoRA \u6743\u91cd\u6587\u4ef6\u7684\u8def\u5f84</li> <li><code>lora_bias</code>: LoRA \u7684\u504f\u5dee</li> <li><code>q_lora</code>: \u662f\u5426\u4f7f\u7528 Q-LoRA</li> </ul> <pre><code>def maybe_zero_3(param):\n    if hasattr(param, \"ds_id\"):\n        assert param.ds_status == ZeroParamStatus.NOT_AVAILABLE\n        with zero.GatheredParameters([param]):\n            param = param.data.detach().cpu().clone()\n    else:\n        param = param.detach().cpu().clone()\n    return param\n\n\n# Borrowed from peft.utils.get_peft_model_state_dict\ndef get_peft_state_maybe_zero_3(named_params, bias):\n    if bias == \"none\":\n        to_return = {k: t for k, t in named_params if \"lora_\" in k}\n    elif bias == \"all\":\n        to_return = {k: t for k, t in named_params if \"lora_\" in k or \"bias\" in k}\n    elif bias == \"lora_only\":\n        to_return = {}\n        maybe_lora_bias = {}\n        lora_bias_names = set()\n        for k, t in named_params:\n            if \"lora_\" in k:\n                to_return[k] = t\n                bias_name = k.split(\"lora_\")[0] + \"bias\"\n                lora_bias_names.add(bias_name)\n            elif \"bias\" in k:\n                maybe_lora_bias[k] = t\n        for k, t in maybe_lora_bias:\n            if bias_name in lora_bias_names:\n                to_return[bias_name] = t\n    else:\n        raise NotImplementedError\n    to_return = {k: maybe_zero_3(v) for k, v in to_return.items()}\n    return to_return\n\n\ndef safe_save_model_for_hf_trainer(\n    trainer: transformers.Trainer, output_dir: str, bias=\"none\"\n):\n    \"\"\"Collects the state dict and dump to disk.\"\"\"\n    # check if zero3 mode enabled\n    if deepspeed.is_deepspeed_zero3_enabled():\n        state_dict = trainer.model_wrapped._zero3_consolidated_16bit_state_dict()\n    else:\n        if trainer.args.use_lora:\n            state_dict = get_peft_state_maybe_zero_3(\n                trainer.model.named_parameters(), bias\n            )\n        else:\n            state_dict = trainer.model.state_dict()\n    if trainer.args.should_save and trainer.args.local_rank == 0:\n        trainer._save(output_dir, state_dict=state_dict)\n</code></pre> <p>\u65b9\u6cd5 <code>safe_save_model_for_hf_trainer</code> \u901a\u8fc7\u4f7f\u7528 <code>get_peft_state_maybe_zero_3</code> \u6709\u52a9\u4e8e\u89e3\u51b3\u5728\u4fdd\u5b58\u91c7\u7528\u6216\u672a\u91c7\u7528 ZeRO3 \u6280\u672f\u8bad\u7ec3\u7684\u6a21\u578b\u65f6\u9047\u5230\u7684\u95ee\u9898\u3002</p> <pre><code>def preprocess(\n    messages,\n    tokenizer: transformers.PreTrainedTokenizer,\n    max_len: int,\n) -&gt; Dict:\n    \"\"\"Preprocesses the data for supervised fine-tuning.\"\"\"\n\n    texts = []\n    for i, msg in enumerate(messages):\n        texts.append(\n            tokenizer.apply_chat_template(\n                msg,\n                tokenize=True,\n                add_generation_prompt=False,\n                padding=True,\n                max_length=max_len,\n                truncation=True,\n            )\n        )\n    input_ids = torch.tensor(texts, dtype=torch.int)\n    target_ids = input_ids.clone()\n    target_ids[target_ids == tokenizer.pad_token_id] = IGNORE_TOKEN_ID\n    attention_mask = input_ids.ne(tokenizer.pad_token_id)\n\n    return dict(\n        input_ids=input_ids, target_ids=target_ids, attention_mask=attention_mask\n    )\n</code></pre> <p>\u5728\u6570\u636e\u9884\u5904\u7406\u9636\u6bb5\uff0c\u6211\u4eec\u4f7f\u7528 <code>preprocess</code> \u6765\u6574\u7406\u6570\u636e\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6211\u4eec\u5e94\u7528 ChatML \u6a21\u677f\u5bf9\u6587\u672c\u8fdb\u884c\u5904\u7406\u3002\u5982\u679c\u60a8\u503e\u5411\u4e8e\u4f7f\u7528\u5176\u4ed6 Chat \u6a21\u677f\uff0c\u60a8\u4e5f\u53ef\u4ee5\u9009\u62e9\u5176\u4ed6\u7684\uff0c\u4f8b\u5982\uff0c\u4ecd\u7136\u901a\u8fc7 <code>apply_chat_template()</code> \u51fd\u6570\u914d\u5408\u53e6\u4e00\u4e2a tokenizer \u8fdb\u884c\u5e94\u7528\u3002Chat \u6a21\u677f\u5b58\u50a8\u5728 HF \u4ed3\u5e93\u4e2d\u7684 tokenizer_config.json \u6587\u4ef6\u4e2d\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5c06\u6bcf\u4e2a\u6837\u672c\u7684\u5e8f\u5217\u586b\u5145\u5230\u6700\u5927\u957f\u5ea6\uff0c\u4ee5\u4fbf\u4e8e\u8bad\u7ec3\u3002</p> <pre><code>class SupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(\n        self, raw_data, tokenizer: transformers.PreTrainedTokenizer, max_len: int\n    ):\n        super(SupervisedDataset, self).__init__()\n\n        rank0_print(\"Formatting inputs...\")\n        messages = [example[\"messages\"] for example in raw_data]\n        data_dict = preprocess(messages, tokenizer, max_len)\n\n        self.input_ids = data_dict[\"input_ids\"]\n        self.target_ids = data_dict[\"target_ids\"]\n        self.attention_mask = data_dict[\"attention_mask\"]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, i) -&gt; Dict[str, torch.Tensor]:\n        return dict(\n            input_ids=self.input_ids[i],\n            labels=self.labels[i],\n            attention_mask=self.attention_mask[i],\n        )\n\n\nclass LazySupervisedDataset(Dataset):\n    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n\n    def __init__(\n        self, raw_data, tokenizer: transformers.PreTrainedTokenizer, max_len: int\n    ):\n        super(LazySupervisedDataset, self).__init__()\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n        rank0_print(\"Formatting inputs...Skip in lazy mode\")\n        self.tokenizer = tokenizer\n        self.raw_data = raw_data\n        self.cached_data_dict = {}\n\n    def __len__(self):\n        return len(self.raw_data)\n\n    def __getitem__(self, i) -&gt; Dict[str, torch.Tensor]:\n        if i in self.cached_data_dict:\n            return self.cached_data_dict[i]\n\n        ret = preprocess([self.raw_data[i][\"messages\"]], self.tokenizer, self.max_len)\n        ret = dict(\n            input_ids=ret[\"input_ids\"][0],\n            labels=ret[\"target_ids\"][0],\n            attention_mask=ret[\"attention_mask\"][0],\n        )\n        self.cached_data_dict[i] = ret\n\n        return ret\n\n\ndef make_supervised_data_module(\n    tokenizer: transformers.PreTrainedTokenizer,\n    data_args,\n    max_len,\n) -&gt; Dict:\n    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n    dataset_cls = (\n        LazySupervisedDataset if data_args.lazy_preprocess else SupervisedDataset\n    )\n    rank0_print(\"Loading data...\")\n\n    train_data = []\n    with open(data_args.data_path, \"r\") as f:\n        for line in f:\n            train_data.append(json.loads(line))\n    train_dataset = dataset_cls(train_data, tokenizer=tokenizer, max_len=max_len)\n\n    if data_args.eval_data_path:\n        eval_data = []\n        with open(data_args.eval_data_path, \"r\") as f:\n            for line in f:\n                eval_data.append(json.loads(line))\n        eval_dataset = dataset_cls(eval_data, tokenizer=tokenizer, max_len=max_len)\n    else:\n        eval_dataset = None\n\n    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)\n</code></pre> <p>\u7136\u540e\u6211\u4eec\u5229\u7528 <code>make_supervised_data_module</code>\uff0c\u901a\u8fc7\u4f7f\u7528 <code>SupervisedDataset</code> \u6216 <code>LazySupervisedDataset</code> \u6765\u6784\u5efa\u6570\u636e\u96c6\u3002</p> <pre><code>def train():\n    global local_rank\n\n    parser = transformers.HfArgumentParser(\n        (ModelArguments, DataArguments, TrainingArguments, LoraArguments)\n    )\n    (\n        model_args,\n        data_args,\n        training_args,\n        lora_args,\n    ) = parser.parse_args_into_dataclasses()\n\n    # This serves for single-gpu qlora.\n    if (\n        getattr(training_args, \"deepspeed\", None)\n        and int(os.environ.get(\"WORLD_SIZE\", 1)) == 1\n    ):\n        training_args.distributed_state.distributed_type = DistributedType.DEEPSPEED\n\n    local_rank = training_args.local_rank\n\n    device_map = None\n    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n    ddp = world_size != 1\n    if lora_args.q_lora:\n        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)} if ddp else \"auto\"\n        if len(training_args.fsdp) &gt; 0 or deepspeed.is_deepspeed_zero3_enabled():\n            logging.warning(\"FSDP or ZeRO3 is incompatible with QLoRA.\")\n\n    model_load_kwargs = {\n        \"low_cpu_mem_usage\": not deepspeed.is_deepspeed_zero3_enabled(),\n    }\n\n    compute_dtype = (\n        torch.float16\n        if training_args.fp16\n        else (torch.bfloat16 if training_args.bf16 else torch.float32)\n    )\n\n    # Load model and tokenizer\n    config = transformers.AutoConfig.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n    )\n    config.use_cache = False\n\n    model = AutoModelForCausalLM.from_pretrained(\n        model_args.model_name_or_path,\n        config=config,\n        cache_dir=training_args.cache_dir,\n        device_map=device_map,\n        quantization_config=BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=compute_dtype,\n        )\n        if training_args.use_lora and lora_args.q_lora\n        else None,\n        **model_load_kwargs,\n    )\n    tokenizer = AutoTokenizer.from_pretrained(\n        model_args.model_name_or_path,\n        cache_dir=training_args.cache_dir,\n        model_max_length=training_args.model_max_length,\n        padding_side=\"right\",\n        use_fast=False,\n    )\n\n    if training_args.use_lora:\n        lora_config = LoraConfig(\n            r=lora_args.lora_r,\n            lora_alpha=lora_args.lora_alpha,\n            target_modules=lora_args.lora_target_modules,\n            lora_dropout=lora_args.lora_dropout,\n            bias=lora_args.lora_bias,\n            task_type=\"CAUSAL_LM\",\n        )\n        if lora_args.q_lora:\n            model = prepare_model_for_kbit_training(\n                model, use_gradient_checkpointing=training_args.gradient_checkpointing\n            )\n\n        model = get_peft_model(model, lora_config)\n\n        # Print peft trainable params\n        model.print_trainable_parameters()\n\n        if training_args.gradient_checkpointing:\n            model.enable_input_require_grads()\n\n    # Load data\n    data_module = make_supervised_data_module(\n        tokenizer=tokenizer, data_args=data_args, max_len=training_args.model_max_length\n    )\n\n    # Start trainer\n    trainer = Trainer(\n        model=model, tokenizer=tokenizer, args=training_args, **data_module\n    )\n\n    # `not training_args.use_lora` is a temporary workaround for the issue that there are problems with\n    # loading the checkpoint when using LoRA with DeepSpeed.\n    # Check this issue https://github.com/huggingface/peft/issues/746 for more information.\n    if (\n        list(pathlib.Path(training_args.output_dir).glob(\"checkpoint-*\"))\n        and not training_args.use_lora\n    ):\n        trainer.train(resume_from_checkpoint=True)\n    else:\n        trainer.train()\n    trainer.save_state()\n\n    safe_save_model_for_hf_trainer(\n        trainer=trainer, output_dir=training_args.output_dir, bias=lora_args.lora_bias\n    )\n</code></pre> <p><code>train</code> \u65b9\u6cd5\u662f\u8bad\u7ec3\u8fc7\u7a0b\u7684\u5173\u952e\u6240\u5728\u3002\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u5b83\u4f1a\u901a\u8fc7 <code>AutoTokenizer.from_pretrained()</code> \u548c <code>AutoModelForCausalLM.from_pretrained()</code> \u6765\u52a0\u8f7dtokenizer \u548c\u6a21\u578b\u3002\u5982\u679c\u4f7f\u7528\u4e86 LoRA\uff0c\u8be5\u65b9\u6cd5\u5c06\u4f1a\u7528 <code>LoraConfig</code> \u521d\u59cb\u5316 LoRA \u914d\u7f6e\u3002\u82e5\u5e94\u7528 Q-LoRA\uff0c\u5219\u5e94\u5f53\u91c7\u7528 <code>prepare_model_for_kbit_training</code>\u3002\u8bf7\u6ce8\u610f\uff0c\u76ee\u524d\u8fd8\u4e0d\u652f\u6301\u5bf9 LoRA \u7684\u7eed\u8bad\uff08resume\uff09\u3002\u63a5\u4e0b\u6765\u7684\u4efb\u52a1\u5c31\u4ea4\u7ed9 trainer \u5904\u7406\u3002</p>"},{"location":"v2/sft_llama_factory/","title":"LLama Factory \u5fae\u8c03","text":"<p>\u672c\u5c4a\u5c06\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 LLaMA-Factory \u5fae\u8c03 Qwen2 \u6a21\u578b\u3002\u672c\u811a\u672c\u5305\u542b\u5982\u4e0b\u7279\u70b9\uff1a</p> <ul> <li>\u652f\u6301\u5355\u5361\u548c\u591a\u5361\u5206\u5e03\u5f0f\u8bad\u7ec3</li> <li>\u652f\u6301\u5168\u53c2\u6570\u5fae\u8c03\u3001LoRA\u3001Q-LoRA \u548c DoRA\u3002</li> </ul>"},{"location":"v2/sft_llama_factory/#1","title":"1. \u5b89\u88c5","text":"<p>\u6839\u636e LLaMA-Factory \u5b98\u65b9\u6307\u5f15\u6784\u5efa\u597d\u4f60\u7684\u73af\u5883:</p> <pre><code>pip install deepspeed\npip install flash-attn --no-build-isolation\n</code></pre> <p>\u4f7f\u7528 Flash Attention2</p> <p>\u5982\u679c\u4f7f\u7528 Flash Attention 2\uff0c\u8bf7\u786e\u4fdd CUDA \u7248\u672c\u5728 11.6 \u4ee5\u4e0a\u3002</p>"},{"location":"v2/sft_llama_factory/#2","title":"2. \u51c6\u5907\u6570\u636e","text":"<p>LLaMA-Factory \u652f\u6301\u4ee5 alpaca \u6216 sharegpt \u683c\u5f0f\u7684\u6570\u636e\u96c6\uff0c\u5728 data \u6587\u4ef6\u5939\u4e2d\u63d0\u4f9b\u4e86\u591a\u4e2a\u8bad\u7ec3\u6570\u636e\u96c6 (1) \u3002\u5982\u679c\u60a8\u6253\u7b97\u4f7f\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff0c\u8bf7\u6309\u7167\u4ee5\u4e0b\u65b9\u5f0f\u51c6\u5907\u60a8\u7684\u6570\u636e\u96c6\u3002</p> <ol> <li>\u81ea\u5b9a\u4e49\u6570\u636e\u4ee5 json \u683c\u5f0f\u8fdb\u884c\u7ec4\u7ec7\uff0c\u5e76\u653e\u5165 data \u6587\u4ef6\u5939\u4e2d\u3002</li> </ol> \u81ea\u5b9a\u4e49\u6570\u636e\u96c6 AlpacaShareGPT <pre><code>[\n  {\n    \"instruction\": \"user instruction (required)\",\n    \"input\": \"user input (optional)\",\n    \"output\": \"model response (required)\",\n    \"system\": \"system prompt (optional)\",\n    \"history\": [\n      [\"user instruction in the first round (optional)\", \"model response in the first round (optional)\"],\n      [\"user instruction in the second round (optional)\", \"model response in the second round (optional)\"]\n    ]\n  }\n]\n</code></pre> <pre><code>[\n  {\n    \"conversations\": [\n      {\n        \"from\": \"human\",\n        \"value\": \"user instruction\"\n      },\n      {\n        \"from\": \"gpt\",\n        \"value\": \"model response\"\n      }\n    ],\n    \"system\": \"system prompt (optional)\",\n    \"tools\": \"tool description (optional)\"\n  }\n]\n</code></pre> <p>\u5728 data/dataset_info.json \u6587\u4ef6\u4e2d\u63d0\u4f9b\u60a8\u7684\u6570\u636e\u96c6\u5b9a\u4e49\uff0c\u5e76\u91c7\u7528\u4ee5\u4e0b\u683c\u5f0f\uff1a</p> dataset_info.json AlpacaShareGPT <pre><code>\"dataset_name\": {\n  \"file_name\": \"dataset_name.json\",\n  \"columns\": {\n    \"prompt\": \"instruction\",\n    \"query\": \"input\",\n    \"response\": \"output\",\n    \"system\": \"system\",\n    \"history\": \"history\"\n  }\n}\n</code></pre> <pre><code>\"dataset_name\": {\n  \"file_name\": \"dataset_name.json\",\n  \"formatting\": \"sharegpt\",\n  \"columns\": {\n    \"messages\": \"conversations\",\n    \"system\": \"system\",\n    \"tools\": \"tools\"\n  },\n  \"tags\": {\n    \"role_tag\": \"from\",\n    \"content_tag\": \"value\",\n    \"user_tag\": \"user\",\n    \"assistant_tag\": \"assistant\"\n  }\n}\n</code></pre>"},{"location":"v2/sft_llama_factory/#3","title":"3. \u8bad\u7ec3","text":"<pre><code>DISTRIBUTED_ARGS=\"\n    --nproc_per_node $NPROC_PER_NODE \\\n    --nnodes $NNODES \\\n    --node_rank $NODE_RANK \\\n    --master_addr $MASTER_ADDR \\\n    --master_port $MASTER_PORT\n  \"\n\ntorchrun $DISTRIBUTED_ARGS src/train.py \\\n    --deepspeed $DS_CONFIG_PATH \\\n    --stage sft \\\n    --do_train \\\n    --use_fast_tokenizer \\\n    --flash_attn \\\n    --model_name_or_path $MODEL_PATH \\\n    --dataset your_dataset \\\n    --template qwen \\\n    --finetuning_type lora \\\n    --lora_target q_proj,v_proj\\\n    --output_dir $OUTPUT_PATH \\\n    --overwrite_cache \\\n    --overwrite_output_dir \\\n    --warmup_steps 100 \\\n    --weight_decay 0.1 \\\n    --per_device_train_batch_size 4 \\\n    --gradient_accumulation_steps 4 \\\n    --ddp_timeout 9000 \\\n    --learning_rate 5e-6 \\\n    --lr_scheduler_type cosine \\\n    --logging_steps 1 \\\n    --cutoff_len 4096 \\\n    --save_steps 1000 \\\n    --plot_loss \\\n    --num_train_epochs 3 \\\n    --bf16\n</code></pre> <p>\u5982\u679c\u8981\u8c03\u6574\u8bad\u7ec3\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4fee\u6539\u8bad\u7ec3\u547d\u4ee4\u4e2d\u7684\u53c2\u6570\u6765\u8c03\u6574\u8d85\u53c2\u6570\u3002\u5176\u4e2d\u4e00\u4e2a\u9700\u8981\u6ce8\u610f\u7684\u53c2\u6570\u662f <code>cutoff_len</code>\uff0c\u5b83\u4ee3\u8868\u8bad\u7ec3\u6570\u636e\u7684\u6700\u5927\u957f\u5ea6\u3002\u901a\u8fc7\u63a7\u5236\u8fd9\u4e2a\u53c2\u6570\uff0c\u53ef\u4ee5\u907f\u514d\u51fa\u73b0OOM\uff08\u5185\u5b58\u6ea2\u51fa\uff09\u9519\u8bef\u3002</p>"},{"location":"v2/sft_llama_factory/#4-lora","title":"4. \u5408\u5e76 LoRA","text":"<p>\u5982\u679c\u4f60\u4f7f\u7528 LoRA \u8bad\u7ec3\u6a21\u578b\uff0c\u53ef\u80fd\u9700\u8981\u5c06 adapter \u53c2\u6570\u5408\u5e76\u5230\u4e3b\u5206\u652f\u4e2d\u3002\u8bf7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u4ee5\u6267\u884c LoRA adapter \u7684\u5408\u5e76\u64cd\u4f5c\u3002</p> <pre><code>CUDA_VISIBLE_DEVICES=0 llamafactory-cli export \\\n    --model_name_or_path path_to_base_model \\\n    --adapter_name_or_path path_to_adapter \\\n    --template qwen \\\n    --finetuning_type lora \\\n    --export_dir path_to_export \\\n    --export_size 2 \\\n    --export_legacy_format False\n</code></pre>"},{"location":"v2/skypilot/","title":"SkyPilot","text":"<p>SkyPilot \u662f\u4e00\u4e2a\u53ef\u4ee5\u5728\u4efb\u4f55\u4e91\u4e0a\u8fd0\u884c LLM\u3001AI \u5e94\u7528\u4ee5\u53ca\u6279\u91cf\u4efb\u52a1\u7684\u6846\u67b6\uff0c\u65e8\u5728\u5b9e\u73b0\u6700\u5927\u7a0b\u5ea6\u7684\u6210\u672c\u8282\u7701\u3001\u6700\u9ad8\u7684 GPU \u53ef\u7528\u6027\u4ee5\u53ca\u53d7\u7ba1\u7406\u7684\u6267\u884c\u8fc7\u7a0b\u3002\u5176\u7279\u6027\u5305\u62ec\uff1a</p> <ul> <li>\u901a\u8fc7\u8de8\u533a\u57df\u548c\u8de8\u4e91\u5145\u5206\u5229\u7528\u591a\u4e2a\u8d44\u6e90\u6c60\uff0c\u4ee5\u83b7\u5f97\u6700\u4f73\u7684 GPU \u53ef\u7528\u6027\u3002</li> <li>\u628a\u8d39\u7528\u964d\u5230\u6700\u4f4e \u2014\u2014 SkyPilot \u5728\u5404\u533a\u57df\u548c\u4e91\u5e73\u53f0\u4e2d\u4e3a\u60a8\u6311\u9009\u6700\u4fbf\u5b9c\u7684\u8d44\u6e90\u3002\u65e0\u9700\u4efb\u4f55\u6258\u7ba1\u89e3\u51b3\u65b9\u6848\u7684\u989d\u5916\u52a0\u4ef7\u3002</li> <li>\u5c06\u670d\u52a1\u6269\u5c55\u5230\u591a\u4e2a\u526f\u672c\u4e0a\uff0c\u6240\u6709\u526f\u672c\u901a\u8fc7\u5355 \u2014\u2014 endpoint \u5bf9\u5916\u63d0\u4f9b\u670d\u52a1</li> <li>\u6240\u6709\u5185\u5bb9\u5747\u4fdd\u5b58\u5728\u60a8\u7684\u4e91\u8d26\u6237\u4e2d\uff08\u5305\u62ec\u60a8\u7684\u865a\u62df\u673a\u548c bucket \uff09</li> <li>\u5b8c\u5168\u79c1\u5bc6\uff0c\u6ca1\u6709\u5176\u4ed6\u4eba\u80fd\u770b\u5230\u60a8\u7684\u804a\u5929\u8bb0\u5f55</li> </ul>"},{"location":"v2/skypilot/#1","title":"1. \u5b89\u88c5","text":"<p>\u6211\u4eec\u5efa\u8bae\u60a8\u6309\u7167 \u6307\u793a \u5b89\u88c5 SkyPilot\uff0c\u901a\u5e38\u6211\u4eec\u4f7f\u7528 <code>pip</code> \u5c31\u53ef\u4ee5\u5feb\u901f\u5b89\u88c5:</p> <pre><code>pip install \"skypilot-nightly[aws,gcp]\"\n</code></pre> <p>\u968f\u540e\uff0c\u60a8\u9700\u8981\u7528\u5982\u4e0b\u547d\u4ee4\u786e\u8ba4\u662f\u5426\u80fd\u4f7f\u7528\u4e91\uff1a</p> <pre><code>sky check\n</code></pre> <p>\u6216\u8005\u6211\u4eec\u4e5f\u53ef\u4ee5\u4f7f\u7528\u5b98\u65b9\u63d0\u4f9b\u7684 docker \u955c\u50cf\uff0c\u53ef\u4ee5\u81ea\u52a8\u514b\u9686 SkyPilot \u7684\u4e3b\u5206\u652f\uff1a</p> <pre><code>docker run --platform linux/amd64 \\\n  -td --rm --name sky \\\n  -v \"$HOME/.sky:/root/.sky:rw\" \\\n  -v \"$HOME/.aws:/root/.aws:rw\" \\\n  -v \"$HOME/.config/gcloud:/root/.config/gcloud:rw\" \\\n  berkeleyskypilot/skypilot-nightly\n\ndocker exec -it sky /bin/bash\n</code></pre>"},{"location":"v2/skypilot/#2-qwen2-72b-instruct","title":"2. \u8fd0\u884c Qwen2-72B-Instruct","text":"<p>serve-72b.yaml \u4e2d\u5217\u51fa\u4e86\u652f\u6301\u7684 GPU\uff0c\u6211\u4eec\u53ef\u4f7f\u7528\u914d\u5907\u8fd9\u7c7b GPU \u7684\u5355\u4e2a\u8fd0\u7b97\u5b9e\u4f8b\u6765\u90e8\u7f72 Qwen2-72B-Instruct \u670d\u52a1\u3002\u8be5\u670d\u52a1\u7531 vLLM \u642d\u5efa\uff0c\u5e76\u4e0e OpenAI API \u517c\u5bb9\u3002</p> <p><pre><code>sky launch -c qwen serve-72b.yaml\n</code></pre> \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u53ef\u4ee5\u5411 endpoint \u53d1\u9001\u7eed\u5199\u8bf7\u6c42\uff1a</p> <pre><code>IP=$(sky status --ip qwen)\n\ncurl -L http://$IP:8000/v1/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n      \"model\": \"Qwen/Qwen2-72B-Instruct\",\n      \"prompt\": \"My favorite food is\",\n      \"max_tokens\": 512\n  }' | jq -r '.choices[0].text'\n</code></pre> <p>\u4e5f\u53ef\u4ee5\u5411\u8be5 endpoint \u53d1\u9001\u5bf9\u8bdd\u7eed\u5199\u8bf7\u6c42\uff1a</p> <pre><code>curl -L http://$IP:8000/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n      \"model\": \"Qwen/Qwen2-72B-Instruct\",\n      \"messages\": [\n        {\n          \"role\": \"system\",\n          \"content\": \"You are a helpful and honest chat expert.\"\n        },\n        {\n          \"role\": \"user\",\n          \"content\": \"What is the best food?\"\n        }\n      ],\n      \"max_tokens\": 512\n  }' | jq -r '.choices[0].message.content'\n</code></pre>"},{"location":"v2/skypilot/#3","title":"3. \u6269\u5c55\u670d\u52a1\u89c4\u6a21","text":"<p>\u4f7f\u7528 SkyPilot Serve \u6269\u5c55 Qwen \u7684\u670d\u52a1\u89c4\u6a21\u975e\u5e38\u5bb9\u6613\uff0c\u53ea\u9700\u8fd0\u884c\uff1a</p> <pre><code>sky serve up -n qwen ./serve-72b.yaml\n</code></pre> <p>\u8fd9\u5c06\u542f\u52a8\u670d\u52a1\uff0c\u4f7f\u7528\u591a\u4e2a\u526f\u672c\u90e8\u7f72\u5728\u6700\u7ecf\u6d4e\u7684\u53ef\u7528\u4f4d\u7f6e\u548c\u52a0\u901f\u5668\u4e0a\u3002SkyServe \u5c06\u81ea\u52a8\u7ba1\u7406\u8fd9\u4e9b\u526f\u672c\uff0c\u76d1\u63a7\u5176\u5065\u5eb7\u72b6\u51b5\uff0c\u6839\u636e\u8d1f\u8f7d\u8fdb\u884c\u81ea\u52a8\u4f38\u7f29\uff0c\u5e76\u5728\u5fc5\u8981\u65f6\u91cd\u542f\u5b83\u4eec\u3002\u670d\u52a1\u5c06\u8fd4\u56de\u4e00\u4e2a endpoint \uff0c\u6240\u6709\u53d1\u9001\u81f3\u8be5 endpoint \u7684\u8bf7\u6c42\u90fd\u5c06\u88ab\u8def\u7531\u81f3\u5c31\u7eea\u72b6\u6001\u7684\u526f\u672c\u3002</p> <p>\u8fd0\u884c\u5982\u4e0b\u547d\u4ee4\u68c0\u67e5\u670d\u52a1\u7684\u72b6\u6001\uff1a</p> <pre><code>sky serve status qwen\n</code></pre> <p>\u5f88\u5feb\uff0c\u60a8\u5c06\u770b\u5230\u5982\u4e0b\u8f93\u51fa\uff1a</p> <pre><code>Services\nNAME        VERSION  UPTIME  STATUS        REPLICAS  ENDPOINT\nQwen  1        -       READY         2/2       3.85.107.228:30002\n\nService Replicas\nSERVICE_NAME  ID  VERSION  IP  LAUNCHED    RESOURCES                   STATUS REGION\nQwen          1   1        -   2 mins ago  1x Azure({'A100-80GB': 8}) READY  eastus\nQwen          2   1        -   2 mins ago  1x GCP({'L4': 8})          READY  us-east4-a\n</code></pre> <p>\u5982\u4e0b\u6240\u793a\uff1a\u8be5\u670d\u52a1\u73b0\u7531\u4e24\u4e2a\u526f\u672c\u63d0\u4f9b\u652f\u6301\uff0c\u4e00\u4e2a\u4f4d\u4e8e Azure \u5e73\u53f0\uff0c\u53e6\u4e00\u4e2a\u4f4d\u4e8e GCP \u5e73\u53f0\u3002\u540c\u65f6\uff0c\u5df2\u4e3a\u670d\u52a1\u9009\u62e9\u4e91\u670d\u52a1\u5546\u63d0\u4f9b\u7684 \u6700\u7ecf\u6d4e\u5b9e\u60e0 \u7684\u52a0\u901f\u5668\u7c7b\u578b\u3002\u8fd9\u6837\u65e2\u6700\u5927\u9650\u5ea6\u5730\u63d0\u5347\u4e86\u670d\u52a1\u7684\u53ef\u7528\u6027\uff0c\u53c8\u5c3d\u53ef\u80fd\u964d\u4f4e\u4e86\u6210\u672c\u3002</p> <p>\u8981\u8bbf\u95ee\u6a21\u578b\uff0c\u6211\u4eec\u4f7f\u7528\u5e26\u6709 <code>curl -L</code> \uff08\u7528\u4e8e\u8ddf\u968f\u91cd\u5b9a\u5411\uff09\uff0c\u5c06\u8bf7\u6c42\u53d1\u9001\u5230 endpoint \uff1a</p> <pre><code>ENDPOINT=$(sky serve status --endpoint qwen)\n\ncurl -L http://$ENDPOINT/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n      \"model\": \"Qwen/Qwen2-72B-Instruct\",\n      \"messages\": [\n        {\n          \"role\": \"system\",\n          \"content\": \"You are a helpful and honest code assistant expert in Python.\"\n        },\n        {\n          \"role\": \"user\",\n          \"content\": \"Show me the python code for quick sorting a list of integers.\"\n        }\n      ],\n      \"max_tokens\": 512\n  }' | jq -r '.choices[0].message.content'\n</code></pre>"},{"location":"v2/skypilot/#4-chat-gui-qwen2","title":"4. \u4f7f\u7528 Chat GUI \u8c03\u7528 Qwen2","text":"<p>\u53ef\u4ee5\u901a\u8fc7 FastChat \u6765\u4f7f\u7528 GUI \u8c03\u7528 Qwen2 \u7684\u670d\u52a1\u3002\u5f00\u542f\u4e00\u4e2a Chat Web UI\uff1a</p> <pre><code>sky launch -c qwen-gui ./gui.yaml --env ENDPOINT=$(sky serve status --endpoint qwen)\n</code></pre> <p>\u968f\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u8fd4\u56de\u7684 gradio \u94fe\u63a5\u6765\u8bbf\u95ee GUI \uff1a</p> <pre><code>| INFO | stdout | Running on public URL: https://6141e84201ce0bb4ed.gradio.live\n</code></pre> <p>\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u4e0d\u540c\u7684\u6e29\u5ea6\u548c <code>top_p</code> \u503c\u6765\u5c1d\u8bd5\u53d6\u5f97\u66f4\u597d\u7684\u7ed3\u679c\u3002</p>"},{"location":"v2/skypilot/#5","title":"5. \u603b\u7ed3","text":"<p>\u901a\u8fc7 SkyPilot\uff0c\u4f60\u53ef\u4ee5\u8f7b\u677e\u5730\u5728\u4efb\u4f55\u4e91\u4e0a\u90e8\u7f72 Qwen2\u3002\u6211\u4eec\u5efa\u8bae\u60a8\u9605\u8bfb \u5b98\u65b9\u6587\u6863 \u4e86\u89e3\u66f4\u591a\u7528\u6cd5\u548c\u6700\u65b0\u8fdb\u5c55\u3002</p>"},{"location":"v2/speed_benchmark/","title":"\u6548\u7387\u8bc4\u4f30","text":"<p>\u672c\u6587\u4ecb\u7ecd Qwen2 \u6a21\u578b\uff08\u539f\u59cb\u6a21\u578b\u548c\u91cf\u5316\u6a21\u578b\uff09\u7684\u6548\u7387\u6d4b\u8bd5\u7ed3\u679c\uff0c\u5305\u62ec\u63a8\u7406\u901f\u5ea6\u4e0e\u4e0d\u540c\u4e0a\u4e0b\u6587\u957f\u5ea6\u65f6\u7684\u663e\u5b58\u5360\u7528\u3002</p> Transformers \u548c vLLM \u6d4b\u8bd5\u73af\u5883 HF TransformersvLLM <ul> <li>NVIDIA A100 80GB</li> <li>CUDA 11.8</li> <li>Pytorch 2.1.2+cu118</li> <li>Flash Attention 2.3.3</li> <li>Transformers 4.38.2</li> <li>AutoGPTQ 0.7.1</li> <li>AutoAWQ 0.2.4</li> </ul> <ul> <li>NVIDIA A100 80GB</li> <li>CUDA 11.8</li> <li>Pytorch 2.3.0+cu118</li> <li>Flash Attention 2.5.6</li> <li>Transformers 4.40.1</li> <li>vLLM 0.4.2</li> </ul> <p>\u6ce8\u610f\u4e8b\u9879</p> <ul> <li>\u4e3a\u4fdd\u8bc1 \u4f7f\u7528 GPU \u6570\u91cf\u5c3d\u53ef\u80fd\u5c11\uff0c\u8fd9\u91cc <code>batch_size</code> \u8bbe\u7f6e\u4e3a 1\u3002</li> <li>\u6d4b\u8bd5\u751f\u6210 2048 tokens \u65f6\u7684\u901f\u5ea6\u4e0e\u663e\u5b58\u5360\u7528\uff0c\u8f93\u5165\u957f\u5ea6\u5206\u522b\u4e3a 1\u30016144\u300114336\u300130720\u300163488\u3001129024 tokens\u3002(\u8d85\u8fc7 32K \u957f\u5ea6\u4ec5\u6709 Qwen2-72B-Instruct \u4e0e Qwen2-7B-Instruct \u652f\u6301)</li> <li>\u5bf9\u4e8e vLLM\uff0c\u7531\u4e8e GPU \u663e\u5b58\u9884\u5206\u914d\uff0c\u5b9e\u9645\u663e\u5b58\u4f7f\u7528\u96be\u4ee5\u8bc4\u4f30\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u7edf\u4e00\u8bbe\u5b9a\u4e3a <code>gpu_memory_utilization=0.9</code>\u3001<code>max_model_len=32768</code>\u3001<code>enforce_eager=False</code>\u3002</li> </ul>"},{"location":"v2/speed_benchmark/#1-qwen2-transformers","title":"1. Qwen2 Transformers \u6a21\u578b\u6548\u7387\u5bf9\u6bd4","text":"Qwen2 \u4e0d\u540c Transformers \u6a21\u578b\u7684\u6548\u7387\u5bf9\u6bd4 0.5B1.5B7B57B-A14B72B \u8f93\u5165\u957f\u5ea6 \u91cf\u5316 GPU \u6570\u91cf \u901f\u5ea6(tokens/s) \u5360\u7528\u663e\u5b58(GB) 1 BF16 1 49.94 1.17 GPTQ-Int8 1 36.35 0.85 GPTQ-Int4 1 49.56 0.68 AWQ 1 38.78 0.68 6144 BF16 1 50.83 6.42 GPTQ-Int8 1 36.56 6.09 GPTQ-Int4 1 49.63 5.93 AWQ 1 38.73 5.92 14336 BF16 1 49.56 13.48 GPTQ-Int8 1 36.23 13.15 GPTQ-Int4 1 48.68 12.97 AWQ 1 38.94 12.99 30720 BF16 1 49.25 27.61 GPTQ-Int8 1 34.61 27.28 GPTQ-Int4 1 48.18 27.12 AWQ 1 38.19 27.11 \u8f93\u5165\u957f\u5ea6 \u91cf\u5316 GPU \u6570\u91cf \u901f\u5ea6(tokens/s) \u5360\u7528\u663e\u5b58(GB) 1 BF16 1 40.89 3.44 GPTQ-Int8 1 31.51 2.31 GPTQ-Int4 1 49.56 1.67 AWQ 1 33.62 1.64 6144 BF16 1 40.86 8.74 GPTQ-Int8 1 31.31 7.59 GPTQ-Int4 1 42.78 6.95 AWQ 1 32.90 6.92 14336 BF16 1 40.08 15.92 GPTQ-Int8 1 31.19 14.79 GPTQ-Int4 1 42.25 14.14 AWQ 1 33.24 14.12 30720 BF16 1 34.09 30.31 GPTQ-Int8 1 28.52 29.18 GPTQ-Int4 1 31.30 28.54 AWQ 1 32.16 28.51 \u8f93\u5165\u957f\u5ea6 \u91cf\u5316 GPU \u6570\u91cf \u901f\u5ea6(tokens/s) \u5360\u7528\u663e\u5b58(GB) 1 BF16 1 37.97 14.92 GPTQ-Int8 1 30.85 8.97 GPTQ-Int4 1 36.17 6.06 AWQ 1 33.08 5.93 6144 BF16 1 34.74 20.26 GPTQ-Int8 1 31.13 14.31 GPTQ-Int4 1 33.34 11.40 AWQ 1 30.86 11.27 14336 BF16 1 26.63 27.71 GPTQ-Int8 1 24.58 21.76 GPTQ-Int4 1 25.81 18.86 AWQ 1 27.61 18.72 30720 BF16 1 17.49 42.62 GPTQ-Int8 1 16.69 36.67 GPTQ-Int4 1 17.17 33.76 AWQ 1 17.87 33.63 \u8f93\u5165\u957f\u5ea6 \u91cf\u5316 GPU \u6570\u91cf \u901f\u5ea6(tokens/s) \u5360\u7528\u663e\u5b58(GB) 1 BF16 2 4.76 110.29 GPTQ-Int4 1 5.55 30.38 6144 BF16 2 4.90 117.80 GPTQ-Int4 1 5.44 35.67 14336 BF16 1 4.58 128.17 GPTQ-Int4 1 5.31 43.11 30720 BF16 2 4.12 163.77 GPTQ-Int4 1 4.72 58.01 \u8f93\u5165\u957f\u5ea6 \u91cf\u5316 GPU \u6570\u91cf \u901f\u5ea6(tokens/s) \u5360\u7528\u663e\u5b58(GB) 1 BF16 2 7.45 134.74 GPTQ-Int8 2 7.30 71.00 GPTQ-Int4 1 9.05 41.80 AWQ 1 9.96 41.31 6144 BF16 2 5.99 144.38 GPTQ-Int8 2 5.93 80.60 GPTQ-Int4 1 6.79 47.90 AWQ 1 7.49 47.42 14336 BF16 3 4.12 169.93 GPTQ-Int8 2 4.43 95.14 GPTQ-Int4 1 4.87 57.79 AWQ 1 5.23 57.30 30720 BF16 3 2.86 209.03 GPTQ-Int8 2 2.83 124.20 GPTQ-Int4 2 3.02 107.94 AWQ 2 1.85 88.60"},{"location":"v2/speed_benchmark/#2-qwen2-vllm","title":"2. Qwen2 vLLM \u6a21\u578b\u6548\u7387\u5bf9\u6bd4","text":"<p>\u6570\u636e\u7531 vLLM \u541e\u5410\u91cf\u6d4b\u8bd5\u811a\u672c\u6d4b\u5f97\uff0c\u53ef\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u590d\u73b0:</p> <pre><code>python vllm/benchmarks/benchmark_throughput.py \\\n  --input-len 1000 \\\n  --output-len 100 \\\n  --model &lt;model_path&gt; \\\n  --num-prompts &lt;number of prompts&gt; \\\n  --enforce-eager \\\n  -tp 2\n</code></pre> Qwen2 \u4e0d\u540c vLLM \u6a21\u578b\u7684\u6548\u7387\u5bf9\u6bd4 0.5B1.5B vLLM7B57B-A14B72B \u8f93\u5165\u957f\u5ea6 \u91cf\u5316 GPU \u6570\u91cf \u901f\u5ea6(tokens/s) 1 BF16 1 270.49 GPTQ-Int8 1 235.95 GPTQ-Int4 1 240.07 AWQ 1 233.31 6144 BF16 1 256.16 GPTQ-Int8 1 224.30 GPTQ-Int4 1 226.41 AWQ 1 222.83 14336 BF16 1 108.89 GPTQ-Int8 1 108.10 GPTQ-Int4 1 106.51 AWQ 1 104.16 30720 BF16 1 97.20 GPTQ-Int8 1 94.49 GPTQ-Int4 1 93.94 AWQ 1 92.23 \u8f93\u5165\u957f\u5ea6 \u91cf\u5316 GPU \u6570\u91cf \u901f\u5ea6(tokens/s) 1 BF16 1 175.55 GPTQ-Int8 1 172.28 GPTQ-Int4 1 184.58 AWQ 1 170.87 6144 BF16 1 166.23 GPTQ-Int8 1 164.32 GPTQ-Int4 1 174.04 AWQ 1 162.81 14336 BF16 1 83.67 GPTQ-Int8 1 98.63 GPTQ-Int4 1 97.65 AWQ 1 92.48 30720 BF16 1 77.69 GPTQ-Int8 1 86.42 GPTQ-Int4 1 87.49 AWQ 1 82.88 \u8f93\u5165\u957f\u5ea6 \u91cf\u5316 GPU \u6570\u91cf \u901f\u5ea6(tokens/s) 1 BF16 1 80.45 GPTQ-Int8 1 114.32 GPTQ-Int4 1 143.40 AWQ 1 96.65 6,144 BF16 1 76.41 GPTQ-Int8 1 107.02 GPTQ-Int4 1 131.55 AWQ 1 91.38 14,336 BF16 1 66.54 GPTQ-Int8 1 89.72 GPTQ-Int4 1 97.93 AWQ 1 76.87 30,720 BF16 1 55.83 GPTQ-Int8 1 71.58 GPTQ-Int4 1 81.48 AWQ 1 63.62 63,488 BF16 1 41.20 GPTQ-Int8 1 49.37 GPTQ-Int4 1 54.12 AWQ 1 45.89 129,024 BF16 1 25.01 GPTQ-Int8 1 27.73 GPTQ-Int4 1 29.39 AWQ 1 27.13 \u8f93\u5165\u957f\u5ea6 \u91cf\u5316 GPU \u6570\u91cf \u901f\u5ea6(tokens/s) 1 BF16 2 31.44 6144 BF16 2 31.77 14336 BF16 2 21.25 30720 BF16 2 20.24 \u8f93\u5165\u957f\u5ea6 \u91cf\u5316 GPU \u6570\u91cf \u901f\u5ea6(tokens/s) 1 BF16 2 17.68 BF16 4 30.01 GPTQ-Int8 2 27.56 GPTQ-Int4 1 29.60 GPTQ-Int4 2 42.82 AWQ 2 27.73 6,144 BF16 4 27.98 GPTQ-Int8 2 25.46 GPTQ-Int4 1 25.16 GPTQ-Int4 2 38.23 AWQ 2 25.77 14,336 BF16 4 21.81 GPTQ-Int8 2 22.71 GPTQ-Int4 2 26.64 AWQ 2 21.50 30,720 BF16 4 19.43 GPTQ-Int8 2 18.69 GPTQ-Int4 2 23.12 AWQ 2 18.09 63,488 BF16 4 17.46 GPTQ-Int8 2 15.30 GPTQ-Int4 2 13.23 AWQ 2 13.14 129,024 BF16 4 11.70 GPTQ-Int8 4 12.94 GPTQ-Int4 2 8.33 AWQ 2 7.78 <p>\u4e0d\u540c\u53c2\u6570\u8bbe\u5b9a\u4e0b\uff0c\u8868\u73b0\u53ef\u80fd\u4f1a\u51fa\u73b0\u5dee\u5f02\u3002</p> <p>\u6df7\u5408\u4e13\u5bb6\u6a21\u578b (Mixture-of-Experts, MoE) \u4e0e\u7a20\u5bc6\u6a21\u578b\u76f8\u6bd4\uff0c\u5f53\u6279\u5927\u5c0f\u8f83\u5927\u65f6\uff0c\u541e\u5410\u91cf\u66f4\u5927\u3002\u4e0b\u8868\u5c55\u793a\u4e86\u6709\u5173\u6570\u636e\uff1a</p> \u6a21\u578b \u91cf\u5316 \u63d0\u793a\u8bcd QPS Tokens/s Qwen1.5-32B-Chat BF16 100 6.68 7343.56 Qwen2-57B-A14B-Instruct BF16 100 4.81 5291.15 Qwen1.5-32B-Chat BF16 1000 7.99 8791.35 Qwen2-57B-A14B-Instruct BF16 1000 5.18 5698.37"},{"location":"v2/text-generate-web-ui/","title":"Text Generate Web UI","text":"<p>Text Generation Web UI \uff08\u7b80\u79f0TGW\uff09\u662f\u4e00\u6b3e\u6d41\u884c\u7684\u6587\u672c\u751f\u6210 Web \u754c\u9762\u5de5\u5177\uff0c\u7c7b\u4f3c\u4e8e AUTOMATIC1111/stable-diffusion-webui \u3002\u5b83\u62e5\u6709\u591a\u4e2a\u4ea4\u4e92\u754c\u9762\uff0c\u5e76\u652f\u6301\u591a\u79cd\u6a21\u578b\u540e\u7aef\uff0c\u5305\u62ec Transformers\u3001llama.cpp\uff08\u901a\u8fc7 llama-cpp-python \u5b9e\u73b0\uff09\u3001ExLlamaV2 \u3001AutoGPTQ \u3001AutoAWQ \u3001GPTQ-for-LLaMa \u3001CTransformers \u4ee5\u53ca QuIP# \u3002\u5728\u672c\u8282\u4e2d\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u5728\u672c\u5730\u73af\u5883\u4e2d\u4f7f\u7528 TGW \u8fd0\u884c Qwen2\u3002</p> <p>\u5efa\u8bae\u60a8\u4f7f\u7528\u5176\u4ed6 WEB UI</p> <p>TGW \u7684 WEB \u754c\u9762\u8fd8\u662f\u6bd4\u8f83\u7b80\u964b\u7684\uff0c\u5982\u679c\u4f60\u719f\u6089 Streamlit\uff0c\u5efa\u8bae\u60a8\u81ea\u5df1\u7f16\u5199\u4e00\u4e2a\u989c\u503c\u66f4\u9ad8\u7684 UI \u754c\u9762\u3002</p> <p>\u6700\u7b80\u5355\u7684\u8fd0\u884c TGW \u7684\u65b9\u6cd5\u662f\u4f7f\u7528\u4ed3\u5e93 \u4e2d\u63d0\u4f9b\u7684 Shell \u811a\u672c\u3002</p> <pre><code>git clone https://github.com/oobabooga/text-generation-webui\ncd text-generation-webui\n</code></pre> <p>\u4f60\u53ef\u4ee5\u6839\u636e\u4f60\u7684\u64cd\u4f5c\u7cfb\u7edf\u76f4\u63a5\u8fd0\u884c\u76f8\u5e94\u7684\u811a\u672c:</p> \u4e0d\u540c\u7cfb\u7edf\u8fd0\u884c\u811a\u672c WindowsLinuxMacOS <pre><code>./start_windows.bat\n</code></pre> <pre><code>./start_linunx.sh\n</code></pre> <pre><code>./start_macos.sh\n</code></pre> <p>\u53e6\u5916\uff0c\u4f60\u4e5f\u53ef\u4ee5\u9009\u62e9\u624b\u52a8\u5728conda\u73af\u5883\u4e2d\u5b89\u88c5\u6240\u9700\u7684\u4f9d\u8d56\u9879\uff0c\u8fd9\u91cc\u4ee5 MacOS \u7cfb\u7edf\uff08Apple Silicon\uff09\u4e3a\u4f8b\u8fdb\u884c\u5b9e\u8df5\u64cd\u4f5c\u3002</p> <pre><code>conda create -n textgen python=3.11\nconda activate textgen\npip install torch torchvision torchaudio\npip install -r requirements_apple_silicon.txt\n</code></pre> <p>\u5bf9\u4e8e requirements \u4e2d\u7684 <code>bitsandbytes</code> \u548c <code>llama-cpp-python</code>\uff0c\u6211\u4eec\u5efa\u8bae\u60a8\u76f4\u63a5\u901a\u8fc7 <code>pip</code> \u8fdb\u884c\u5b89\u88c5\u3002\u4f46\u662f\uff0c\u6682\u65f6\u8bf7\u4e0d\u8981\u4f7f\u7528GGUF\uff0c\u56e0\u4e3a\u5176\u4e0e TGW \u914d\u5408\u65f6\u7684\u6027\u80fd\u8868\u73b0\u4e0d\u4f73\u3002\u5728\u5b8c\u6210\u6240\u9700\u5305\u7684\u5b89\u88c5\u4e4b\u540e\uff0c\u60a8\u9700\u8981\u51c6\u5907\u6a21\u578b\uff0c\u5c06\u6a21\u578b\u6587\u4ef6\u6216\u76ee\u5f55\u653e\u5728 ./models \u6587\u4ef6\u5939\u4e2d\u3002\u4f8b\u5982\uff0c\u60a8\u5e94\u6309\u7167\u4ee5\u4e0b\u65b9\u5f0f\u5c06 Qwen2-7B-Instruct \u6a21\u578b\u653e\u7f6e\u5230\u76f8\u5e94\u4f4d\u7f6e\u3002</p> \u5b58\u653e\u4f4d\u7f6e<pre><code>text-generation-webui\n\u251c\u2500\u2500 models\n\u2502   \u251c\u2500\u2500 Qwen2-7B-Instruct\n\u2502   \u2502   \u251c\u2500\u2500 config.json\n\u2502   \u2502   \u251c\u2500\u2500 generation_config.json\n\u2502   \u2502   \u251c\u2500\u2500 model-00001-of-00004.safetensor\n\u2502   \u2502   \u251c\u2500\u2500 model-00002-of-00004.safetensor\n\u2502   \u2502   \u251c\u2500\u2500 model-00003-of-00004.safetensor\n\u2502   \u2502   \u251c\u2500\u2500 model-00004-of-00004.safetensor\n\u2502   \u2502   \u251c\u2500\u2500 model.safetensor.index.json\n\u2502   \u2502   \u251c\u2500\u2500 merges.txt\n\u2502   \u2502   \u251c\u2500\u2500 tokenizer_config.json\n\u2502   \u2502   \u2514\u2500\u2500 vocab.json\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u901a\u8fc7\u8fd0\u884c <code>python server.py</code> \u6765\u542f\u52a8\u7f51\u9875\u670d\u52a1\uff0c\u5e76\u8bbf\u95ee http://localhost:7860 \u6765\u4e0e Qwen2 \u8fdb\u884c\u4ea4\u4e92\u3002</p>"},{"location":"v2/tgi/","title":"TGI","text":"<p>TGI\uff08Hugging Face \u7684 Text Generation Inference\uff09\u662f\u4e00\u4e2a\u4e13\u4e3a\u90e8\u7f72\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u800c\u8bbe\u8ba1\u7684\u751f\u4ea7\u7ea7\u6846\u67b6\uff0c\u5b83\u63d0\u4f9b\u4e86\u6d41\u7545\u7684\u90e8\u7f72\u4f53\u9a8c\uff0c\u5e76\u7a33\u5b9a\u652f\u6301\u5982\u4e0b\u7279\u6027\uff1a</p> <ul> <li>\u63a8\u6d4b\u89e3\u7801 (Speculative Decoding) \uff1a\u63d0\u5347\u751f\u6210\u901f\u5ea6\u3002</li> <li>\u5f20\u91cf\u5e76\u884c (Tensor Parallelism) \uff1a\u9ad8\u6548\u591a\u5361\u90e8\u7f72\u3002</li> <li>\u6d41\u5f0f\u751f\u6210 (Token Streaming) \uff1a\u652f\u6301\u6301\u7eed\u6027\u751f\u6210\u6587\u672c\u3002</li> <li>\u7075\u6d3b\u7684\u786c\u4ef6\u652f\u6301\uff1a\u4e0e AMD\u3001Gaudi \u548c AWS Inferentia \u65e0\u7f1d\u8854\u63a5\u3002</li> </ul>"},{"location":"v2/tgi/#1","title":"1. \u5b89\u88c5","text":"<p>TGI \u6700\u7b80\u5355\u7684\u4f7f\u7528\u65b9\u5f0f\u662f Docker \u955c\u50cf\uff0c\u5f53\u7136\u4e5f\u53ef\u901a\u8fc7 Conda \u5b9e\u673a\u5b89\u88c5\u6216\u642d\u5efa\u670d\u52a1\u3002\u8bf7\u53c2\u8003 TGI \u5b89\u88c5\u6307\u5357\u4ee5\u53ca\u547d\u4ee4\u884c\u5de5\u5177 \u4e86\u89e3\u8be6\u7ec6\u8bf4\u660e\u3002</p>"},{"location":"v2/tgi/#2-tgi-qwen2","title":"2.\u901a\u8fc7 TGI \u90e8\u7f72 Qwen2","text":"<p>\u5728\u7ec8\u7aef\u4e2d\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u6ce8\u610f\u66ff\u6362 model \u4e3a\u4f60\u60f3\u8981\u4f7f\u7528\u7684 Qwen2 \u6a21\u578b\u3001 volume \u4e3a\u672c\u5730\u7684\u6570\u636e\u8def\u5f84\uff1a</p> TGI \u8fd0\u884c Qwen2-7B-Instruct<pre><code>model=Qwen/Qwen2-7B-Instruct\n# \u907f\u514d\u6bcf\u6b21\u8fd0\u884c\u65f6\u4e0b\u8f7d\u6743\u91cd\uff0c\u8fd9\u91cc\u5c06\u672c\u5730\u76ee\u5f55\u6302\u8f7d\u5230 Docker \u5bb9\u5668\u4e2d\nvolume=$PWD/data\n\ndocker run --gpus all --shm-size 1g -p 8080:80 \\\n    -v $volume:/data \\ \n    ghcr.io/huggingface/text-generation-inference:2.0 \\\n    --model-id $model\n</code></pre>"},{"location":"v2/tgi/#2-tgi-api","title":"2. \u4f7f\u7528 TGI API","text":"<p>\u4e00\u65e6\u6210\u529f\u90e8\u7f72\uff0cAPI \u5c06\u4e8e\u9009\u5b9a\u7684\u6620\u5c04\u7aef\u53e3 (8080) \u63d0\u4f9b\u670d\u52a1\uff0cTGI \u63d0\u4f9b\u4e86\u7b80\u5355\u76f4\u63a5\u7684 API \u652f\u6301\u6d41\u5f0f\u751f\u6210\uff0c\u540c\u65f6\u4e5f\u652f\u6301 OpenAI \u98ce\u683c\u7684 API\uff1a</p> TGI API \u7684\u4f7f\u7528 \u6d41\u5f0f APIOpenAIPython <pre><code>curl http://localhost:8080/generate_stream \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"inputs\": \"Tell me something about large language models.\",\n    \"parameters\": {\"max_new_tokens\": 512}\n  }'\n</code></pre> <pre><code>// `model` \u5b57\u6bb5\u4e0d\u4f1a\u88ab TGI \u8bc6\u522b\uff0c\u8fd9\u91cc\u53ef\u4f20\u5165\u4efb\u610f\u503c\u3002\ncurl http://localhost:8080/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"model\": \"\",\n    \"messages\": [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"}\n      ],\n    \"max_tokens\": 512\n  }'\n</code></pre> <pre><code>from openai import OpenAI\n\n\nclient = OpenAI(\n    base_url=\"http://localhost:8080/v1/\",\n    api_key=\"\",\n)\nchat_completion = client.chat.completions.create(\n    model=\"\",  # (1)!\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"},\n    ],\n    stream=True,\n    max_tokens=512,\n)\n\nfor message in chat_completion:\n    print(message.choices[0].delta.content, end=\"\")\n</code></pre> <ol> <li><code>model</code> \u5b57\u6bb5\u4e0d\u4f1a\u88ab TGI \u8bc6\u522b\uff0c\u6211\u4eec\u53ef\u4f20\u5165\u4efb\u610f\u503c\u3002</li> </ol>"},{"location":"v2/tgi/#3","title":"3. \u91cf\u5316","text":""},{"location":"v2/tgi/#31-gptq-awq","title":"3.1 GPTQ \u4e0e AWQ","text":"<p>GPTQ \u4e0e AWQ \u5747\u4f9d\u8d56\u6570\u636e\u8fdb\u884c\u91cf\u5316\uff0cQwen2 \u63d0\u4f9b\u4e86\u4e0d\u540c\u5927\u5c0f\u4e14\u9884\u5148\u91cf\u5316\u597d\u7684\u6a21\u578b\u3002\u5f53\u7136\u6211\u4eec\u4e5f\u53ef\u4ee5\u4f7f\u7528\u81ea\u5df1\u7684\u4e1a\u52a1\u6570\u636e\u96c6\u81ea\u884c\u91cf\u5316\uff0c\u4ece\u800c\u53d6\u5f97\u66f4\u597d\u6548\u679c\u3002</p> <pre><code>model=Qwen/Qwen2-7B-Instruct-GPTQ-Int4\nvolume=$PWD/data\n\ndocker run --gpus all --shm-size 1g -p 8080:80 \\\n  -v $volume:/data \\\n  ghcr.io/huggingface/text-generation-inference:2.0 \\\n  --model-id $model \\\n  --quantize gptq  # (1)!\n</code></pre> <ol> <li>\u4f7f\u7528 GPTQ \u91cf\u5316\u65b9\u5f0f\uff0c\u5982\u679c\u4f7f\u7528 AWQ \u91cf\u5316\u6a21\u578b\uff0c\u5219\u4f7f\u7528 <code>--quantize awq</code>\u3002</li> </ol>"},{"location":"v2/tgi/#32","title":"3.2 \u4e0d\u4f9d\u8d56\u6570\u636e\u7684\u91cf\u5316\u65b9\u6848","text":"<p>EETQ \u662f\u4e00\u79cd\u4e0d\u4f9d\u8d56\u6570\u636e\u7684\u91cf\u5316\u65b9\u6848\uff0c\u53ef\u76f4\u63a5\u7528\u4e8e\u4efb\u610f\u6a21\u578b\u3002\u8bf7\u6ce8\u610f\uff0c\u6211\u4eec\u9700\u8981\u4f20\u5165\u539f\u59cb\u6a21\u578b\uff0c\u5e76\u4f7f\u7528 <code>--quantize eetq</code> \u6807\u5fd7\u3002</p> <pre><code>model=Qwen/Qwen2-7B-Instruct\nvolume=$PWD/data\n\ndocker run --gpus all --shm-size 1g -p 8080:80 \\\n  -v $volume:/data \\\n  ghcr.io/huggingface/text-generation-inference:2.0 \\\n  --model-id $model \\\n  --quantize eetq\n</code></pre>"},{"location":"v2/tgi/#33","title":"3.3 \u5ef6\u8fdf\u6307\u6807","text":"<p>\u4ee5\u4e0b\u4e3a Qwen2-7B-Instruct \u91cf\u5316\u6a21\u578b\u5728 4090 \u4e0a\u7684 token/s \u7ed3\u679c\uff1a</p> <ul> <li>GPTQ int4: 6.8ms</li> <li>AWQ int4: 7.9ms</li> <li>EETQ int8: 9.7ms</li> </ul>"},{"location":"v2/tgi/#4","title":"4. \u591a\u5361\u90e8\u7f72","text":"<p>\u4f7f\u7528 <code>--num-shard</code> \u6307\u5b9a\u663e\u5361\u6570\u91cf\uff0c\u8bf7\u52a1\u5fc5\u4f20\u5165 <code>--shm-size 1g</code> \u8ba9 NCCL \u53d1\u6325\u6700\u597d\u6027\u80fd (\u8bf4\u660e) \uff1a</p> <pre><code>model=Qwen/Qwen2-7B-Instruct\nvolume=$PWD/data\n\ndocker run --gpus all --shm-size 1g -p 8080:80 \\\n  -v $volume:/data \\\n  ghcr.io/huggingface/text-generation-inference:2.0 \\\n  --model-id $model \\\n  --num-shard 2\n</code></pre>"},{"location":"v2/tgi/#5","title":"5. \u63a8\u6d4b\u6027\u89e3\u7801","text":"<p>\u63a8\u6d4b\u6027\u89e3\u7801 (Speculative Decoding) \u901a\u8fc7\u9884\u5148\u63a8\u6d4b\u4e0b\u4e00 token \u6765\u8282\u7ea6\u6bcf token \u9700\u8981\u7684\u65f6\u95f4\uff0c\u4f7f\u7528 <code>--speculative-decoding</code> \u8bbe\u5b9a\u9884\u5148\u63a8\u6d4b token \u7684\u6570\u91cf \uff08\u9ed8\u8ba4\u4e3a 0\uff0c\u8868\u793a\u4e0d\u9884\u5148\u63a8\u6d4b\uff09\uff1a</p> <pre><code>model=Qwen/Qwen2-7B-Instruct\nvolume=$PWD/data\n\ndocker run --gpus all --shm-size 1g -p 8080:80 \\\n  -v $volume:/data \\\n  ghcr.io/huggingface/text-generation-inference:2.0 \\ \n  --model-id $model \\\n  --speculate 2\n</code></pre> <p>\u4ee5\u4e0b\u662f Qwen2-7B-Instruct \u5728 4090 GPU \u4e0a\u7684\u5b9e\u6d4b\u6307\u6807\uff1a</p> <ul> <li>\u65e0\u9884\u5148\u63a8\u6d4b\uff08\u9ed8\u8ba4\uff09\uff1a17.4ms</li> <li>\u9884\u5148\u63a8\u6d4b (n=2)\uff1a16.6ms</li> </ul> <p>\u672c\u4f8b\u4e3a\u4ee3\u7801\u751f\u6210\uff0c\u63a8\u6d4b\u6027\u89e3\u7801\u76f8\u6bd4\u4e8e\u9ed8\u8ba4\u914d\u7f6e\u53ef\u52a0\u901f 10%\u3002\u63a8\u6d4b\u6027\u89e3\u7801\u7684\u52a0\u901f\u6548\u679c\u4f9d\u8d56\u4e8e\u4efb\u52a1\u7c7b\u578b\uff0c\u5bf9\u4e8e\u4ee3\u7801\u6216\u91cd\u590d\u6027\u8f83\u9ad8\u7684\u6587\u672c\u751f\u6210\u4efb\u52a1\uff0c\u63d0\u901f\u66f4\u660e\u663e\u3002\u66f4\u591a\u8bf4\u660e\u53ef\u67e5\u9605\u6b64\u6587\u6863\u3002</p>"},{"location":"v2/tgi/#6-hf-inference-endpoints","title":"6. \u4f7f\u7528 HF Inference Endpoints \u96f6\u4ee3\u7801\u90e8\u7f72","text":"<p>\u4f7f\u7528 Hugging Face Inference Endpoints \u4e0d\u8d39\u5439\u7070\u4e4b\u529b\uff1a</p> <ul> <li>GUI interface: https://huggingface.co/inference-endpoints/dedicated</li> <li>Coding interface: https://huggingface.co/blog/tgi-messages-api</li> </ul> <p>\u4e00\u65e6\u90e8\u7f72\u6210\u529f\uff0c\u670d\u52a1\u4f7f\u7528\u4e0e\u672c\u5730\u65e0\u5f02\u3002</p>"},{"location":"v2/tgi/#7","title":"7.\u5e38\u89c1\u95ee\u9898","text":"<p>Qwen2 \u652f\u6301\u957f\u4e0a\u4e0b\u6587\uff0c\u8c28\u614e\u8bbe\u5b9a <code>--max-batch-prefill-tokens</code>\u3001<code>--max-total-tokens</code> \u548c <code>--max-input-tokens</code> \u4ee5\u907f\u514d\u5185\u5b58\u6ea2\u51fa\uff08out-of-memory, OOM\uff09\u3002\u5982 OOM \uff0c\u4f60\u5c06\u5728\u542f\u52a8 TGI \u65f6\u6536\u5230\u9519\u8bef\u63d0\u793a\u3002\u4ee5\u4e0b\u4e3a\u4fee\u6539\u8fd9\u4e9b\u53c2\u6570\u7684\u793a\u4f8b\uff1a</p> <pre><code>model=Qwen/Qwen2-7B-Instruct\nvolume=$PWD/data \n\ndocker run --gpus all --shm-size 1g -p 8080:80 \\\n  -v $volume:/data \\\n  ghcr.io/huggingface/text-generation-inference:2.0 \\\n  --model-id $model \\\n  --max-batch-prefill-tokens 4096 \\\n  --max-total-tokens 4096 \\\n  --max-input-tokens 2048\n</code></pre>"},{"location":"v2/vllm/","title":"vLLM","text":"<p>\u6211\u4eec\u5efa\u8bae\u60a8\u5728\u90e8\u7f72 Qwen \u65f6\u5c1d\u8bd5\u4f7f\u7528 vLLM\uff0c\u5b83\u6613\u4e8e\u4f7f\u7528\uff0c\u4e14\u5177\u6709\u6700\u5148\u8fdb\u7684\u670d\u52a1\u541e\u5410\u91cf\u3001\u9ad8\u6548\u7684 \u6ce8\u610f\u529b\u952e\u503c\u5185\u5b58\u7ba1\u7406\uff08\u901a\u8fc7 PagedAttention \u5b9e\u73b0\uff09\u3001\u8fde\u7eed\u6279\u5904\u7406\u8f93\u5165\u8bf7\u6c42\u3001\u4f18\u5316\u7684 CUDA \u5185\u6838\u7b49\u529f\u80fd\u3002</p>"},{"location":"v2/vllm/#1","title":"1. \u5b89\u88c5","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7 <code>pip</code> \u6765\u5b89\u88c5 vLLM\uff1a</p> <pre><code>pip install vLLM&gt;=0.4.0\npip install ray  # \u63a8\u8350\u5b89\u88c5\n</code></pre> <p>\u4f46\u5982\u679c\u4f60\u6b63\u5728\u4f7f\u7528 CUDA 11.8\uff0c\u8bf7\u67e5\u770b\u5b98\u65b9\u6587\u6863\u4e2d\u7684\u6ce8\u610f\u4e8b\u9879\uff0c\u4ee5\u83b7\u53d6\u6709\u5173\u5b89\u88c5\u7684\u5e2e\u52a9\u3002 \u6b64\u5916\uff0c\u5efa\u8bae\u901a\u8fc7\u5b89\u88c5<code>ray</code>\uff0c\u4ee5\u4fbf\u652f\u6301\u5206\u5e03\u5f0f\u670d\u52a1\u3002</p>"},{"location":"v2/vllm/#2","title":"2. \u79bb\u7ebf\u63a8\u7406","text":"<p>vLLM \u652f\u6301 Qwen2 \u6240\u6709\u6a21\u578b\u90fd\uff0c\u79bb\u7ebf\u6279\u91cf\u63a8\u7406\u662f\u6700\u7b80\u5355\u7684\u4f7f\u7528\u65b9\u5f0f\u3002</p> <pre><code>from transformers import AutoTokenizer\nfrom vllm import LLM, SamplingParams\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-7B-Instruct\")\n# `max_tokens` \u7528\u4e8e\u8bbe\u7f6e\u751f\u6210\u7684\u6700\u5927\u957f\u5ea6\nsampling_params = SamplingParams(\n    temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512\n    )\n# \u8fd9\u91cc\u4e5f\u652f\u6301 GPTQ \u548c AWQ \u6a21\u578b\nllm = LLM(model=\"Qwen/Qwen2-7B-Instruct\")\n\nprompt = \"Tell me something about large language models.\"\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": prompt}\n]\ntext = tokenizer.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True\n)\noutputs = llm.generate([text], sampling_params)\n\nfor output in outputs:\n    prompt = output.prompt\n    generated_text = output.outputs[0].text\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n</code></pre>"},{"location":"v2/vllm/#3-openai-apiapi","title":"3. \u9002\u914dOpenAI-API\u7684API\u670d\u52a1","text":"<p>\u501f\u52a9 vLLM\uff0c\u6784\u5efa\u4e00\u4e2a\u4e0e OpenAI API \u517c\u5bb9\u7684\u670d\u52a1\u5341\u5206\u7b80\u4fbf\uff0c\u8be5\u670d\u52a1\u53ef\u4ee5\u4f5c\u4e3a\u5b9e\u73b0 OpenAI API \u534f\u8bae\u7684\u670d\u52a1\u5668\u8fdb\u884c\u90e8\u7f72\u3002</p> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u670d\u52a1\u5c06\u542f\u52a8\u5728 http://localhost:8000 \u7aef\u53e3\u4e0a\uff0c\u53e6\u5916\u53ef\u4ee5\u901a\u8fc7 <code>--host</code> \u548c <code>--port</code> \u53c2\u6570\u6765\u81ea\u5b9a\u4e49\u5730\u5740\u3002\u8fd9\u91cc\u6211\u4eec\u4e0d\u9700\u62c5\u5fc3 Chat \u6a21\u677f\uff0c\u56e0\u4e3a\u9ed8\u8ba4\u4f1a\u4f7f\u7528\u7531 <code>tokenizer</code> \u63d0\u4f9b\u7684 Chat \u6a21\u677f\u3002</p> <pre><code>python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2-7B-Instruct\n</code></pre> <p>\u63a5\u4e0b\u6765\uff0c\u8ba9\u6211\u4eec\u5c1d\u8bd5\u4e0b\u4e0e vLLM \u8fdb\u884c\u4ea4\u4e92\u3002</p> \u4e0e vLLM \u4e0a\u7684 Qwen \u4ea4\u4e92 CURL\u53d1\u9001\u8bf7\u6c42Python\u53d1\u9001\u8bf7\u6c42 <pre><code>curl http://localhost:8000/v1/chat/completions \\\n  -H \"Content-Type: application/json\" \\ \n  -d '{\n    \"model\": \"Qwen/Qwen2-7B-Instruct\",\n    \"messages\": [\n      {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n      {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"}\n    ],\n  }'\n</code></pre> <pre><code>from openai import OpenAI\n\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nchat_response = client.chat.completions.create(\n    model=\"Qwen/Qwen2-7B-Instruct\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Tell me something about large language models.\"},\n    ]\n)\nprint(\"Chat response:\", chat_response)\n</code></pre>"},{"location":"v2/vllm/#4","title":"4.\u591a\u5361\u5206\u5e03\u5f0f\u90e8\u7f72","text":"<p>\u8981\u63d0\u9ad8\u6a21\u578b\u7684\u5904\u7406\u541e\u5410\u91cf\uff0c\u53ef\u4ee5\u901a\u8fc7\u5229\u7528\u66f4\u591a\u7684 GPU \u8bbe\u5907\u6765\u5b9e\u73b0\u5206\u5e03\u5f0f\u670d\u52a1\u3002\u7279\u522b\u662f\u5bf9\u4e8e\u50cf Qwen2-72B-Instruct \u8fd9\u6837\u7684\u5927\u6a21\u578b\uff0c\u5355\u4e2a GPU \u663e\u7136\u65e0\u6cd5\u652f\u6491\u5176\u5728\u7ebf\u670d\u52a1\u3002</p> <p>\u8fd9\u91cc\u6211\u4eec\u901a\u8fc7\u6f14\u793a\u5982\u4f55\u4ec5\u901a\u8fc7\u4f20\u5165\u53c2\u6570 <code>tensor_parallel_size</code> \uff0c\u6765\u4f7f\u7528\u5f20\u91cf\u5e76\u884c\u6765\u8fd0\u884c Qwen2-72B-Instruct \u6a21\u578b\uff1a</p> <pre><code>from vllm import LLM, SamplingParams\n\nllm = LLM(model=\"Qwen/Qwen2-72B-Instruct\", tensor_parallel_size=4)\n</code></pre> <p>\u5f53\u7136\uff0c\u6211\u4eec\u4e5f\u53ef\u4ee5\u901a\u8fc7\u4f20\u9012\u53c2\u6570 <code>--tensor-parallel-size</code> \u6765\u8fd0\u884c\u591a GPU \u670d\u52a1\uff1a</p> <pre><code>python -m vllm.entrypoints.api_server \\\n    --model Qwen/Qwen2-72B-Instruct \\\n    --tensor-parallel-size 4\n</code></pre>"},{"location":"v2/vllm/#5","title":"5. \u90e8\u7f72\u91cf\u5316\u6a21\u578b","text":"<p>vLLM \u652f\u6301\u591a\u79cd\u7c7b\u578b\u7684\u91cf\u5316\u6a21\u578b\uff0c\u4f8b\u5982 AWQ\u3001GPTQ\u3001 SqueezeLLM \u7b49\u3002\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u4e0b\u5982\u4f55\u90e8\u7f72 AWQ \u548c GPTQ \u6a21\u578b\u3002\u4f7f\u7528\u65b9\u6cd5\u4e0e\u4e0a\u8ff0\u57fa\u672c\u76f8\u540c\uff0c\u53ea\u4e0d\u8fc7\u9700\u8981\u989d\u5916\u6307\u5b9a\u4e00\u4e2a\u91cf\u5316\u53c2\u6570 <code>quantization</code>\u3002</p> \u90e8\u7f72 AWQ/GPTQ \u6a21\u578b AWQGPTQ <pre><code>from vllm import LLM, SamplingParams\n\nllm = LLM(model=\"Qwen/Qwen2-72B-Instruct-AWQ\", quantization=\"awq\")\n</code></pre> <pre><code>from vllm import LLM, SamplingParams\n\nllm = LLM(model=\"Qwen/Qwen2-72B-Instruct-GPTQ-Int4\", quantization=\"gptq\")\n</code></pre> <p>\u5f53\u7136\uff0c\u5728\u7ec8\u7aef\u4e2d\u542f\u52a8\u670d\u52a1\u65f6\u4e5f\u53ef\u4ee5\u6dfb\u52a0 <code>--quantization</code> \u53c2\u6570\u3002</p> \u547d\u4ee4\u884c\u4e2d\u542f\u52a8\u91cf\u5316\u6a21\u578b AWQGPTQ <pre><code>python -m vllm.entrypoints.openai.api_server \\\n  --model Qwen/Qwen2-72B-Instruct-AWQ \\\n  --quantization awq\n</code></pre> <pre><code>python -m vllm.entrypoints.openai.api_server \\\n  --model Qwen/Qwen2-72B-Instruct-GPTQ-Int4 \\\n  --quantization gptq\n</code></pre> <p>\u6b64\u5916\uff0cvLLM \u4e5f\u652f\u6301\u5c06 AWQ \u6216 GPTQ \u6a21\u578b\u4e0e KV \u7f13\u5b58\u91cf\u5316\u76f8\u7ed3\u5408\uff0c \u5373FP8 E5M2 KV Cache \u65b9\u6848\u3002</p> <p>vLLM\u4f7f\u7528\u91cf\u5316\u6a21\u578b\u5e76\u7ed3\u5408 KV \u7f13\u5b58</p> Python\u547d\u4ee4\u884c <pre><code>llm = LLM(\n    model=\"Qwen/Qwen2-7B-Instruct-GPTQ-Int8\",\n    quantization=\"gptq\", \n    kv_cache_dtype=\"fp8_e5m2\"\n    )\n</code></pre> <pre><code>python -m vllm.entrypoints.openai.api_server \\\n  --model Qwen/Qwen2-7B-Instruct-GPTQ-Int8 \\\n  --quantization gptq \\\n  --kv-cache-dtype fp8_e5m2\n</code></pre>"},{"location":"v2/vllm/#6","title":"6. \u5e38\u89c1\u95ee\u9898","text":"<p>\u5982\u679c\u9047\u5230\u4ee4\u4eba\u70e6\u607c\u7684 OOM\uff08\u5185\u5b58\u6ea2\u51fa\uff09\u95ee\u9898\uff0c\u8fd9\u91cc\u63a8\u8350\u60a8\u5c1d\u8bd5\u4e24\u4e2a\u53c2\u6570\u8fdb\u884c\u4fee\u590d\u3002</p> <ul> <li>\u7b2c\u4e00\u4e2a\u53c2\u6570\u662f <code>--max-model-len</code>\uff0c\u76ee\u524d\u63d0\u4f9b\u7684\u9ed8\u8ba4\u6700\u5927\u4f4d\u7f6e\u5d4c\u5165\uff08<code>max_position_embedding</code>\uff09\u4e3a 32768\uff0c\u56e0\u6b64\u670d\u52a1\u65f6\u7684\u6700\u5927\u957f\u5ea6\u4e5f\u662f\u8fd9\u4e2a\u503c\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u66f4\u9ad8\u7684\u5185\u5b58\u9700\u6c42\u3002\u5c06\u6b64\u503c\u9002\u5f53\u51cf\u5c0f\u901a\u5e38\u6709\u52a9\u4e8e\u89e3\u51b3 OOM \u95ee\u9898\u3002</li> <li>\u53e6\u4e00\u4e2a\u53c2\u6570\u662f <code>--gpu-memory-utilization</code>\uff0c\u9ed8\u8ba4\u503c\u4e3a 0.9 \uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u5176\u8c03\u9ad8\u4ee5\u5e94\u5bf9 OOM \u95ee\u9898\u3002</li> </ul>"}]}